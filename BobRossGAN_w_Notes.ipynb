{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2WaNhBDwRwTG"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Things im noticing that students keep getting errors from**\n",
        "\n",
        "- Make sure your indentation matches the videos! Otherwise Python wont understand the code properly and it wont print out the summary of the model like in the video!\n"
      ],
      "metadata": {
        "id": "9Igq4uFyBYHO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJp-D51g0IDd"
      },
      "source": [
        "## **1) Importing Python Packages for GAN**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k5mFBuzzl2a"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dense, Reshape, Flatten\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "!mkdir generated_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-eZOzg0X79"
      },
      "source": [
        "## **2) Variables for Neural Networks & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RThZMDruz9cB",
        "outputId": "eb5a056d-94bb-4169-8b2f-f931eab8f517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img_width = 28\n",
        "img_height = 28\n",
        "channels = 1\n",
        "img_shape = (img_width,img_height,channels)\n",
        "\n",
        "latent_dim = 100\n",
        "adam = Adam(lr=0.0001)\n",
        "#adam = stochastic gradient descent algorithm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bcJZZg0cqy"
      },
      "source": [
        "## **3) Building Generator**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdiqZpri0iQh",
        "outputId": "aadd832f-e600-47b1-e510-c18350597c40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_generator():\n",
        "  model = Sequential()\n",
        "  #input layer\n",
        "  #Dense layer is neural network layer that holds a bunch of parameters\n",
        "  model.add(Dense(256, input_dim=latent_dim))\n",
        "  #after every layer you need an activation function. \n",
        "  #relu is default activation function for NNs\n",
        "  #this is a variation of relu called leakyrelu\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  #This is the architecture that generates nice images. It makes the GAN more stable.\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  #Stacking more layers, don't need input_dim after first insertion.\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(1024))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  #output layer\n",
        "  #Final activation layer is always tanh in GANS\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256)              1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 784)               803600    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt6QsJCW0mcI"
      },
      "source": [
        "## **4) Building Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2JzEAPv0lKt",
        "outputId": "f8f35f6c-c0b4-417c-b2dd-a90551ab9af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  #Output layer\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbcKcKmA0q2S"
      },
      "source": [
        "## **5) Connecting Neural Networks to build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ue3TEd0xLy",
        "outputId": "79d59411-ef96-4824-9a7b-e707081376a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "GAN = Sequential()\n",
        "discriminator.trainable = False\n",
        "GAN.add(generator)\n",
        "GAN.add(discriminator)\n",
        "\n",
        "\n",
        "GAN.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "GAN.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 28, 28, 1)         1493520   \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 1)                 533505    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,027,025\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 537,089\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WaNhBDwRwTG"
      },
      "source": [
        "## **6) Outputting Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEJ0WbjRppy"
      },
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    #generate 25 images to fit on a 5 x 5 grid for our animation!\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    global save_name\n",
        "    save_name += 0.00000001\n",
        "    print(\"%.8f\" % save_name)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            # axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "    print('saved')\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE57Lk5V0xs2"
      },
      "source": [
        "## **7) Training GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egSJJvik00Iq",
        "outputId": "43c9b9b3-dab8-4d45-8213-d9c697e4ce62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#make sure batch size is proportional to the amount of data (images) you have.\n",
        "def train(epochs, batch_size=64, save_interval=200):\n",
        "  (X_train, _), (_,_) = mnist.load_data()\n",
        "  #print(X_train.shape)\n",
        "\n",
        "  #normalizing data, use normalizer???\n",
        "  X_train = X_train / 127.5 - 1.\n",
        "\n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    idx = np.random.randint(0,X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    noise = np.random.normal(0,1, (batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    #Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs,valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "    #Taking average of the losses, use mean()?\n",
        "    d_loss = np.add(d_loss_real, d_loss_fake) * 0.5\n",
        "\n",
        "    #New noise to train gan\n",
        "    noise = np.random.normal(0,1, (batch_size, latent_dim))\n",
        "\n",
        "    #Train the GAN ALSO KNOWN AS INVERSE Y LABEL\n",
        "    g_loss= GAN.train_on_batch(noise, valid)\n",
        "\n",
        "    print(\"******* %d %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch,j, d_loss[0], 100* d_loss[1], g_loss))\n",
        "    if(epoch % save_interval) == 0:\n",
        "      save_imgs(epoch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train(30000, batch_size=64, save_interval=200)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "2/2 [==============================] - 4s 5ms/step\n",
            "************ 0 [D Loss: 0.825632, acc] 51.56 [G Loss: 0.573606]\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "0.00000001\n",
            "saved\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1 [D Loss: 0.472811, acc] 85.16 [G Loss: 0.581127]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 2 [D Loss: 0.393633, acc] 78.91 [G Loss: 0.542758]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 3 [D Loss: 0.399443, acc] 64.06 [G Loss: 0.536899]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 4 [D Loss: 0.392746, acc] 64.06 [G Loss: 0.570945]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 5 [D Loss: 0.406358, acc] 61.72 [G Loss: 0.607248]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 6 [D Loss: 0.393188, acc] 64.84 [G Loss: 0.634438]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 7 [D Loss: 0.406518, acc] 64.84 [G Loss: 0.662637]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 8 [D Loss: 0.364220, acc] 69.53 [G Loss: 0.674097]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 9 [D Loss: 0.348312, acc] 75.78 [G Loss: 0.754672]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 10 [D Loss: 0.331546, acc] 81.25 [G Loss: 0.813407]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 11 [D Loss: 0.320657, acc] 82.03 [G Loss: 0.823473]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 12 [D Loss: 0.272028, acc] 92.97 [G Loss: 0.924908]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 13 [D Loss: 0.274860, acc] 91.41 [G Loss: 1.067558]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 14 [D Loss: 0.240312, acc] 94.53 [G Loss: 1.147437]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 15 [D Loss: 0.196022, acc] 99.22 [G Loss: 1.159989]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 16 [D Loss: 0.196382, acc] 99.22 [G Loss: 1.304775]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 17 [D Loss: 0.156872, acc] 100.00 [G Loss: 1.448917]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 18 [D Loss: 0.144634, acc] 100.00 [G Loss: 1.463126]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 19 [D Loss: 0.132565, acc] 100.00 [G Loss: 1.555891]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 20 [D Loss: 0.114630, acc] 100.00 [G Loss: 1.679423]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 21 [D Loss: 0.129683, acc] 100.00 [G Loss: 1.768077]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 22 [D Loss: 0.106016, acc] 100.00 [G Loss: 1.838851]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 23 [D Loss: 0.099923, acc] 100.00 [G Loss: 1.902874]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 24 [D Loss: 0.088327, acc] 100.00 [G Loss: 1.974416]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 25 [D Loss: 0.082906, acc] 100.00 [G Loss: 2.102790]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 26 [D Loss: 0.084300, acc] 100.00 [G Loss: 2.092221]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 27 [D Loss: 0.079340, acc] 100.00 [G Loss: 2.169088]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 28 [D Loss: 0.073631, acc] 100.00 [G Loss: 2.222750]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 29 [D Loss: 0.063735, acc] 100.00 [G Loss: 2.256409]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 30 [D Loss: 0.057911, acc] 100.00 [G Loss: 2.285738]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 31 [D Loss: 0.064886, acc] 100.00 [G Loss: 2.342990]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 32 [D Loss: 0.066723, acc] 100.00 [G Loss: 2.298350]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 33 [D Loss: 0.058485, acc] 100.00 [G Loss: 2.419835]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 34 [D Loss: 0.056704, acc] 100.00 [G Loss: 2.473200]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 35 [D Loss: 0.055117, acc] 100.00 [G Loss: 2.464619]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 36 [D Loss: 0.054824, acc] 100.00 [G Loss: 2.530498]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 37 [D Loss: 0.058208, acc] 100.00 [G Loss: 2.537590]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 38 [D Loss: 0.052801, acc] 100.00 [G Loss: 2.572195]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 39 [D Loss: 0.052089, acc] 100.00 [G Loss: 2.635712]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 40 [D Loss: 0.055522, acc] 100.00 [G Loss: 2.589042]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 41 [D Loss: 0.045604, acc] 100.00 [G Loss: 2.629090]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 42 [D Loss: 0.049532, acc] 100.00 [G Loss: 2.668508]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 43 [D Loss: 0.044408, acc] 100.00 [G Loss: 2.735451]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 44 [D Loss: 0.042523, acc] 100.00 [G Loss: 2.747510]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 45 [D Loss: 0.046886, acc] 100.00 [G Loss: 2.777833]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 46 [D Loss: 0.042791, acc] 100.00 [G Loss: 2.779616]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 47 [D Loss: 0.035964, acc] 100.00 [G Loss: 2.908807]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 48 [D Loss: 0.040686, acc] 100.00 [G Loss: 2.782902]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 49 [D Loss: 0.037425, acc] 100.00 [G Loss: 2.799008]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 50 [D Loss: 0.040936, acc] 100.00 [G Loss: 2.884580]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 51 [D Loss: 0.041086, acc] 100.00 [G Loss: 2.824512]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 52 [D Loss: 0.034611, acc] 100.00 [G Loss: 2.858391]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 53 [D Loss: 0.033780, acc] 100.00 [G Loss: 2.895629]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 54 [D Loss: 0.032934, acc] 100.00 [G Loss: 2.908531]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 55 [D Loss: 0.033595, acc] 100.00 [G Loss: 3.012608]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 56 [D Loss: 0.034656, acc] 100.00 [G Loss: 3.007002]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 57 [D Loss: 0.033359, acc] 100.00 [G Loss: 2.943941]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 58 [D Loss: 0.032002, acc] 100.00 [G Loss: 3.026745]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 59 [D Loss: 0.029742, acc] 100.00 [G Loss: 3.035753]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 60 [D Loss: 0.032172, acc] 100.00 [G Loss: 3.074440]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 61 [D Loss: 0.029884, acc] 100.00 [G Loss: 3.074748]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 62 [D Loss: 0.029822, acc] 100.00 [G Loss: 3.092041]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 63 [D Loss: 0.030178, acc] 100.00 [G Loss: 3.104775]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 64 [D Loss: 0.027960, acc] 100.00 [G Loss: 3.137779]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 65 [D Loss: 0.028965, acc] 100.00 [G Loss: 3.167245]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 66 [D Loss: 0.027175, acc] 100.00 [G Loss: 3.188932]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 67 [D Loss: 0.032081, acc] 100.00 [G Loss: 3.230286]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 68 [D Loss: 0.025934, acc] 100.00 [G Loss: 3.208021]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 69 [D Loss: 0.025477, acc] 100.00 [G Loss: 3.228693]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 70 [D Loss: 0.024444, acc] 100.00 [G Loss: 3.244417]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 71 [D Loss: 0.024955, acc] 100.00 [G Loss: 3.338013]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 72 [D Loss: 0.025058, acc] 100.00 [G Loss: 3.389078]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 73 [D Loss: 0.023385, acc] 100.00 [G Loss: 3.380030]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 74 [D Loss: 0.021829, acc] 100.00 [G Loss: 3.362356]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 75 [D Loss: 0.020993, acc] 100.00 [G Loss: 3.346996]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 76 [D Loss: 0.025056, acc] 100.00 [G Loss: 3.440333]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 77 [D Loss: 0.019593, acc] 100.00 [G Loss: 3.409296]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 78 [D Loss: 0.020778, acc] 100.00 [G Loss: 3.491463]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 79 [D Loss: 0.023149, acc] 100.00 [G Loss: 3.424064]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 80 [D Loss: 0.020632, acc] 100.00 [G Loss: 3.410281]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 81 [D Loss: 0.019868, acc] 100.00 [G Loss: 3.532432]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 82 [D Loss: 0.018460, acc] 100.00 [G Loss: 3.527427]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 83 [D Loss: 0.022426, acc] 100.00 [G Loss: 3.539570]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 84 [D Loss: 0.021069, acc] 100.00 [G Loss: 3.514534]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 85 [D Loss: 0.021508, acc] 100.00 [G Loss: 3.593098]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 86 [D Loss: 0.019150, acc] 100.00 [G Loss: 3.457490]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 87 [D Loss: 0.018553, acc] 100.00 [G Loss: 3.582693]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 88 [D Loss: 0.019974, acc] 100.00 [G Loss: 3.614875]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 89 [D Loss: 0.019631, acc] 100.00 [G Loss: 3.546944]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 90 [D Loss: 0.018769, acc] 100.00 [G Loss: 3.586834]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 91 [D Loss: 0.017312, acc] 100.00 [G Loss: 3.630253]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 92 [D Loss: 0.018999, acc] 100.00 [G Loss: 3.572377]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 93 [D Loss: 0.017626, acc] 100.00 [G Loss: 3.607817]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 94 [D Loss: 0.018342, acc] 100.00 [G Loss: 3.676089]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 95 [D Loss: 0.019123, acc] 100.00 [G Loss: 3.686354]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 96 [D Loss: 0.018875, acc] 100.00 [G Loss: 3.671283]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 97 [D Loss: 0.020515, acc] 100.00 [G Loss: 3.655220]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 98 [D Loss: 0.018749, acc] 100.00 [G Loss: 3.679709]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 99 [D Loss: 0.013571, acc] 100.00 [G Loss: 3.774071]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 100 [D Loss: 0.020029, acc] 100.00 [G Loss: 3.804445]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 101 [D Loss: 0.020006, acc] 100.00 [G Loss: 3.832651]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 102 [D Loss: 0.015330, acc] 100.00 [G Loss: 3.669992]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 103 [D Loss: 0.015652, acc] 100.00 [G Loss: 3.778920]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 104 [D Loss: 0.017826, acc] 100.00 [G Loss: 3.872205]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 105 [D Loss: 0.017131, acc] 100.00 [G Loss: 3.815997]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 106 [D Loss: 0.016802, acc] 100.00 [G Loss: 3.828617]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 107 [D Loss: 0.016508, acc] 100.00 [G Loss: 3.843775]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 108 [D Loss: 0.013482, acc] 100.00 [G Loss: 3.804277]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 109 [D Loss: 0.013227, acc] 100.00 [G Loss: 3.905757]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 110 [D Loss: 0.013316, acc] 100.00 [G Loss: 3.839335]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 111 [D Loss: 0.014842, acc] 100.00 [G Loss: 3.837529]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 112 [D Loss: 0.017507, acc] 100.00 [G Loss: 3.885386]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 113 [D Loss: 0.017141, acc] 100.00 [G Loss: 3.881125]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 114 [D Loss: 0.020804, acc] 100.00 [G Loss: 3.864824]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 115 [D Loss: 0.019841, acc] 100.00 [G Loss: 3.904524]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 116 [D Loss: 0.015331, acc] 100.00 [G Loss: 3.868161]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 117 [D Loss: 0.014554, acc] 100.00 [G Loss: 3.907877]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 118 [D Loss: 0.015721, acc] 100.00 [G Loss: 3.909397]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 119 [D Loss: 0.015723, acc] 100.00 [G Loss: 3.949970]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 120 [D Loss: 0.017647, acc] 100.00 [G Loss: 4.015936]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 121 [D Loss: 0.013177, acc] 100.00 [G Loss: 4.058140]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 122 [D Loss: 0.014276, acc] 100.00 [G Loss: 3.999332]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 123 [D Loss: 0.012560, acc] 100.00 [G Loss: 4.064235]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 124 [D Loss: 0.013332, acc] 100.00 [G Loss: 4.094407]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 125 [D Loss: 0.013160, acc] 100.00 [G Loss: 4.086284]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 126 [D Loss: 0.015446, acc] 100.00 [G Loss: 4.071335]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 127 [D Loss: 0.014108, acc] 100.00 [G Loss: 4.022175]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 128 [D Loss: 0.014437, acc] 100.00 [G Loss: 4.118712]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 129 [D Loss: 0.013950, acc] 100.00 [G Loss: 4.059800]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 130 [D Loss: 0.013909, acc] 100.00 [G Loss: 4.021025]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 131 [D Loss: 0.015497, acc] 100.00 [G Loss: 4.006750]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 132 [D Loss: 0.012039, acc] 100.00 [G Loss: 4.178595]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 133 [D Loss: 0.014269, acc] 100.00 [G Loss: 4.131100]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 134 [D Loss: 0.014596, acc] 100.00 [G Loss: 4.108810]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 135 [D Loss: 0.013938, acc] 100.00 [G Loss: 4.075254]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 136 [D Loss: 0.016636, acc] 100.00 [G Loss: 4.090795]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 137 [D Loss: 0.015757, acc] 100.00 [G Loss: 4.066497]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 138 [D Loss: 0.014640, acc] 100.00 [G Loss: 4.111250]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 139 [D Loss: 0.013188, acc] 100.00 [G Loss: 4.112327]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 140 [D Loss: 0.015256, acc] 100.00 [G Loss: 4.143597]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 141 [D Loss: 0.012421, acc] 100.00 [G Loss: 4.093152]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 142 [D Loss: 0.015117, acc] 100.00 [G Loss: 4.165079]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 143 [D Loss: 0.014328, acc] 100.00 [G Loss: 4.077495]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 144 [D Loss: 0.015178, acc] 100.00 [G Loss: 4.200781]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 145 [D Loss: 0.013142, acc] 100.00 [G Loss: 4.153413]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 146 [D Loss: 0.012280, acc] 100.00 [G Loss: 4.188986]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 147 [D Loss: 0.015967, acc] 100.00 [G Loss: 4.241385]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 148 [D Loss: 0.013073, acc] 100.00 [G Loss: 4.224190]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 149 [D Loss: 0.013261, acc] 100.00 [G Loss: 4.096895]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 150 [D Loss: 0.015472, acc] 100.00 [G Loss: 4.308216]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 151 [D Loss: 0.013668, acc] 100.00 [G Loss: 4.187221]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 152 [D Loss: 0.012732, acc] 100.00 [G Loss: 4.283908]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 153 [D Loss: 0.012794, acc] 100.00 [G Loss: 4.276160]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 154 [D Loss: 0.012564, acc] 100.00 [G Loss: 4.244205]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 155 [D Loss: 0.013685, acc] 100.00 [G Loss: 4.176261]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 156 [D Loss: 0.014917, acc] 100.00 [G Loss: 4.148687]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 157 [D Loss: 0.014714, acc] 100.00 [G Loss: 4.274125]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 158 [D Loss: 0.012635, acc] 100.00 [G Loss: 4.270956]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 159 [D Loss: 0.011764, acc] 100.00 [G Loss: 4.210907]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 160 [D Loss: 0.013293, acc] 100.00 [G Loss: 4.255713]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 161 [D Loss: 0.012315, acc] 100.00 [G Loss: 4.397770]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 162 [D Loss: 0.014195, acc] 100.00 [G Loss: 4.294499]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 163 [D Loss: 0.011287, acc] 100.00 [G Loss: 4.245464]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 164 [D Loss: 0.014227, acc] 100.00 [G Loss: 4.308469]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 165 [D Loss: 0.013446, acc] 100.00 [G Loss: 4.383302]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 166 [D Loss: 0.012641, acc] 100.00 [G Loss: 4.278631]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 167 [D Loss: 0.012491, acc] 100.00 [G Loss: 4.386621]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 168 [D Loss: 0.013718, acc] 100.00 [G Loss: 4.327854]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 169 [D Loss: 0.012597, acc] 100.00 [G Loss: 4.291485]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 170 [D Loss: 0.010969, acc] 100.00 [G Loss: 4.272736]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 171 [D Loss: 0.017779, acc] 100.00 [G Loss: 4.292939]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 172 [D Loss: 0.013102, acc] 100.00 [G Loss: 4.239282]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 173 [D Loss: 0.016645, acc] 100.00 [G Loss: 4.374329]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 174 [D Loss: 0.013209, acc] 100.00 [G Loss: 4.454530]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 175 [D Loss: 0.013024, acc] 100.00 [G Loss: 4.382558]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 176 [D Loss: 0.011736, acc] 100.00 [G Loss: 4.409954]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 177 [D Loss: 0.011615, acc] 100.00 [G Loss: 4.363952]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 178 [D Loss: 0.014853, acc] 100.00 [G Loss: 4.311337]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 179 [D Loss: 0.010200, acc] 100.00 [G Loss: 4.289957]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 180 [D Loss: 0.011551, acc] 100.00 [G Loss: 4.337202]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 181 [D Loss: 0.015181, acc] 100.00 [G Loss: 4.324573]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 182 [D Loss: 0.012868, acc] 100.00 [G Loss: 4.242563]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 183 [D Loss: 0.017067, acc] 100.00 [G Loss: 4.367716]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 184 [D Loss: 0.015646, acc] 100.00 [G Loss: 4.346378]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 185 [D Loss: 0.012112, acc] 100.00 [G Loss: 4.416656]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 186 [D Loss: 0.013948, acc] 100.00 [G Loss: 4.546315]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 187 [D Loss: 0.015640, acc] 100.00 [G Loss: 4.420637]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 188 [D Loss: 0.011700, acc] 100.00 [G Loss: 4.402831]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 189 [D Loss: 0.011826, acc] 100.00 [G Loss: 4.382763]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 190 [D Loss: 0.011547, acc] 100.00 [G Loss: 4.460677]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 191 [D Loss: 0.017343, acc] 100.00 [G Loss: 4.423585]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 192 [D Loss: 0.014741, acc] 100.00 [G Loss: 4.505370]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 193 [D Loss: 0.012207, acc] 100.00 [G Loss: 4.440542]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 194 [D Loss: 0.012320, acc] 100.00 [G Loss: 4.400737]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 195 [D Loss: 0.010544, acc] 100.00 [G Loss: 4.387289]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 196 [D Loss: 0.014819, acc] 100.00 [G Loss: 4.467832]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 197 [D Loss: 0.015362, acc] 100.00 [G Loss: 4.476828]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 198 [D Loss: 0.012405, acc] 100.00 [G Loss: 4.496972]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 199 [D Loss: 0.013560, acc] 100.00 [G Loss: 4.414908]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 200 [D Loss: 0.012747, acc] 100.00 [G Loss: 4.526115]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.00000002\n",
            "saved\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 201 [D Loss: 0.016087, acc] 100.00 [G Loss: 4.589343]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 202 [D Loss: 0.013159, acc] 100.00 [G Loss: 4.319229]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 203 [D Loss: 0.009969, acc] 100.00 [G Loss: 4.459039]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 204 [D Loss: 0.017974, acc] 100.00 [G Loss: 4.555949]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 205 [D Loss: 0.011223, acc] 100.00 [G Loss: 4.490737]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 206 [D Loss: 0.013167, acc] 100.00 [G Loss: 4.438073]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 207 [D Loss: 0.016550, acc] 100.00 [G Loss: 4.429123]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 208 [D Loss: 0.015211, acc] 100.00 [G Loss: 4.403493]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 209 [D Loss: 0.011748, acc] 100.00 [G Loss: 4.379967]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 210 [D Loss: 0.017714, acc] 100.00 [G Loss: 4.549257]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 211 [D Loss: 0.020962, acc] 100.00 [G Loss: 4.433962]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 212 [D Loss: 0.014382, acc] 100.00 [G Loss: 4.575104]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 213 [D Loss: 0.017726, acc] 100.00 [G Loss: 4.471463]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 214 [D Loss: 0.015206, acc] 100.00 [G Loss: 4.416282]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 215 [D Loss: 0.033937, acc] 99.22 [G Loss: 4.413949]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 216 [D Loss: 0.012216, acc] 100.00 [G Loss: 4.526731]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 217 [D Loss: 0.018794, acc] 100.00 [G Loss: 4.623714]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 218 [D Loss: 0.021652, acc] 100.00 [G Loss: 4.657907]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 219 [D Loss: 0.016147, acc] 100.00 [G Loss: 4.578355]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 220 [D Loss: 0.025742, acc] 100.00 [G Loss: 4.616594]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 221 [D Loss: 0.018921, acc] 100.00 [G Loss: 4.467250]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 222 [D Loss: 0.015248, acc] 100.00 [G Loss: 4.432524]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 223 [D Loss: 0.020920, acc] 100.00 [G Loss: 4.504036]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 224 [D Loss: 0.016435, acc] 100.00 [G Loss: 4.535173]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 225 [D Loss: 0.018259, acc] 100.00 [G Loss: 4.523942]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 226 [D Loss: 0.028288, acc] 99.22 [G Loss: 4.535511]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 227 [D Loss: 0.014387, acc] 100.00 [G Loss: 4.504109]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 228 [D Loss: 0.027243, acc] 100.00 [G Loss: 4.679850]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 229 [D Loss: 0.022099, acc] 100.00 [G Loss: 4.608358]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 230 [D Loss: 0.023602, acc] 100.00 [G Loss: 4.590878]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 231 [D Loss: 0.020322, acc] 100.00 [G Loss: 4.381083]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 232 [D Loss: 0.020772, acc] 100.00 [G Loss: 4.534606]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 233 [D Loss: 0.025517, acc] 99.22 [G Loss: 4.301343]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 234 [D Loss: 0.019370, acc] 100.00 [G Loss: 4.639921]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 235 [D Loss: 0.019461, acc] 100.00 [G Loss: 4.542207]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 236 [D Loss: 0.027103, acc] 100.00 [G Loss: 4.505882]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 237 [D Loss: 0.029814, acc] 100.00 [G Loss: 4.647089]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 238 [D Loss: 0.025415, acc] 100.00 [G Loss: 4.578575]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 239 [D Loss: 0.029289, acc] 100.00 [G Loss: 4.469243]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 240 [D Loss: 0.034635, acc] 98.44 [G Loss: 4.378792]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 241 [D Loss: 0.024661, acc] 100.00 [G Loss: 4.594399]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 242 [D Loss: 0.026290, acc] 99.22 [G Loss: 4.601454]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 243 [D Loss: 0.026014, acc] 100.00 [G Loss: 4.302627]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 244 [D Loss: 0.035257, acc] 99.22 [G Loss: 4.309584]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 245 [D Loss: 0.026094, acc] 100.00 [G Loss: 4.404125]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 246 [D Loss: 0.028545, acc] 100.00 [G Loss: 4.530038]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 247 [D Loss: 0.031753, acc] 99.22 [G Loss: 4.295888]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 248 [D Loss: 0.042675, acc] 100.00 [G Loss: 4.315989]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 249 [D Loss: 0.043999, acc] 100.00 [G Loss: 4.273308]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 250 [D Loss: 0.028703, acc] 100.00 [G Loss: 4.388118]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 251 [D Loss: 0.031707, acc] 99.22 [G Loss: 4.326396]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 252 [D Loss: 0.024881, acc] 100.00 [G Loss: 4.289256]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 253 [D Loss: 0.040897, acc] 100.00 [G Loss: 4.198161]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 254 [D Loss: 0.036250, acc] 99.22 [G Loss: 4.222229]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 255 [D Loss: 0.063175, acc] 98.44 [G Loss: 4.240571]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 256 [D Loss: 0.039335, acc] 100.00 [G Loss: 4.373375]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 257 [D Loss: 0.058925, acc] 97.66 [G Loss: 4.698769]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 258 [D Loss: 0.061143, acc] 98.44 [G Loss: 4.438848]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 259 [D Loss: 0.055223, acc] 100.00 [G Loss: 4.152459]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 260 [D Loss: 0.067215, acc] 97.66 [G Loss: 4.298332]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 261 [D Loss: 0.061634, acc] 99.22 [G Loss: 4.345969]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 262 [D Loss: 0.034686, acc] 100.00 [G Loss: 4.457917]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 263 [D Loss: 0.056272, acc] 100.00 [G Loss: 4.148642]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 264 [D Loss: 0.051696, acc] 98.44 [G Loss: 3.936519]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 265 [D Loss: 0.072860, acc] 96.88 [G Loss: 3.963662]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 266 [D Loss: 0.080611, acc] 96.09 [G Loss: 4.010999]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 267 [D Loss: 0.069384, acc] 97.66 [G Loss: 4.318420]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 268 [D Loss: 0.045882, acc] 100.00 [G Loss: 4.227946]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 269 [D Loss: 0.146362, acc] 92.97 [G Loss: 3.669266]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 270 [D Loss: 0.059795, acc] 98.44 [G Loss: 3.455626]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 271 [D Loss: 0.132994, acc] 92.97 [G Loss: 3.463918]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 272 [D Loss: 0.132348, acc] 95.31 [G Loss: 4.027626]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 273 [D Loss: 0.032170, acc] 100.00 [G Loss: 4.641150]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 274 [D Loss: 0.119683, acc] 96.09 [G Loss: 4.006324]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 275 [D Loss: 0.071690, acc] 98.44 [G Loss: 3.441116]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 276 [D Loss: 0.081220, acc] 96.88 [G Loss: 3.316165]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 277 [D Loss: 0.134731, acc] 92.97 [G Loss: 3.468770]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 278 [D Loss: 0.052895, acc] 99.22 [G Loss: 4.072140]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 279 [D Loss: 0.059982, acc] 99.22 [G Loss: 4.228043]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 280 [D Loss: 0.076618, acc] 97.66 [G Loss: 3.904176]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 281 [D Loss: 0.058846, acc] 99.22 [G Loss: 3.460962]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 282 [D Loss: 0.093123, acc] 97.66 [G Loss: 3.155605]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 283 [D Loss: 0.116401, acc] 93.75 [G Loss: 3.550627]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 284 [D Loss: 0.053490, acc] 98.44 [G Loss: 3.599212]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 285 [D Loss: 0.063493, acc] 100.00 [G Loss: 3.717044]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 286 [D Loss: 0.118958, acc] 98.44 [G Loss: 3.562342]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 287 [D Loss: 0.162482, acc] 93.75 [G Loss: 3.333844]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 288 [D Loss: 0.089682, acc] 98.44 [G Loss: 3.412387]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 289 [D Loss: 0.111865, acc] 96.88 [G Loss: 3.667642]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 290 [D Loss: 0.098769, acc] 97.66 [G Loss: 3.478112]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 291 [D Loss: 0.073537, acc] 98.44 [G Loss: 3.196404]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 292 [D Loss: 0.101443, acc] 97.66 [G Loss: 3.178003]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 293 [D Loss: 0.072259, acc] 97.66 [G Loss: 3.078195]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 294 [D Loss: 0.088013, acc] 98.44 [G Loss: 3.408183]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 295 [D Loss: 0.073092, acc] 100.00 [G Loss: 3.689597]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 296 [D Loss: 0.118355, acc] 96.88 [G Loss: 3.387965]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 297 [D Loss: 0.087867, acc] 97.66 [G Loss: 3.310329]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 298 [D Loss: 0.071525, acc] 98.44 [G Loss: 3.188287]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 299 [D Loss: 0.064439, acc] 98.44 [G Loss: 3.246429]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 300 [D Loss: 0.095548, acc] 96.88 [G Loss: 3.431758]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 301 [D Loss: 0.096747, acc] 98.44 [G Loss: 3.598937]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 302 [D Loss: 0.114599, acc] 96.88 [G Loss: 3.452676]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 303 [D Loss: 0.096503, acc] 96.88 [G Loss: 3.443984]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 304 [D Loss: 0.081478, acc] 97.66 [G Loss: 3.340156]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 305 [D Loss: 0.104658, acc] 95.31 [G Loss: 3.359956]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 306 [D Loss: 0.118716, acc] 96.09 [G Loss: 3.403603]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 307 [D Loss: 0.071388, acc] 99.22 [G Loss: 3.308621]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 308 [D Loss: 0.121125, acc] 97.66 [G Loss: 3.477588]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 309 [D Loss: 0.108388, acc] 97.66 [G Loss: 3.756995]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 310 [D Loss: 0.091385, acc] 96.88 [G Loss: 3.691605]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 311 [D Loss: 0.152099, acc] 93.75 [G Loss: 3.623842]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 312 [D Loss: 0.099019, acc] 96.88 [G Loss: 3.114697]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 313 [D Loss: 0.123712, acc] 96.09 [G Loss: 3.082643]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 314 [D Loss: 0.152891, acc] 94.53 [G Loss: 3.454515]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 315 [D Loss: 0.066990, acc] 98.44 [G Loss: 3.514245]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 316 [D Loss: 0.101851, acc] 98.44 [G Loss: 3.817661]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 317 [D Loss: 0.121310, acc] 96.09 [G Loss: 3.290477]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 318 [D Loss: 0.106027, acc] 97.66 [G Loss: 3.348030]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 319 [D Loss: 0.123981, acc] 93.75 [G Loss: 3.203822]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 320 [D Loss: 0.075264, acc] 98.44 [G Loss: 3.778234]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 321 [D Loss: 0.105491, acc] 97.66 [G Loss: 3.331274]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 322 [D Loss: 0.112624, acc] 96.09 [G Loss: 3.594204]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 323 [D Loss: 0.153246, acc] 92.97 [G Loss: 3.292508]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 324 [D Loss: 0.175647, acc] 92.97 [G Loss: 3.210887]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 325 [D Loss: 0.117665, acc] 94.53 [G Loss: 3.508276]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 326 [D Loss: 0.199522, acc] 91.41 [G Loss: 3.250436]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 327 [D Loss: 0.130897, acc] 96.88 [G Loss: 3.358902]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 328 [D Loss: 0.147338, acc] 92.19 [G Loss: 3.292401]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 329 [D Loss: 0.102862, acc] 96.09 [G Loss: 3.594121]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 330 [D Loss: 0.123658, acc] 95.31 [G Loss: 3.498832]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 331 [D Loss: 0.085471, acc] 99.22 [G Loss: 3.352587]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 332 [D Loss: 0.149272, acc] 93.75 [G Loss: 3.435395]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 333 [D Loss: 0.074603, acc] 99.22 [G Loss: 3.305578]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 334 [D Loss: 0.138572, acc] 94.53 [G Loss: 3.157074]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 335 [D Loss: 0.089493, acc] 98.44 [G Loss: 3.255293]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 336 [D Loss: 0.110893, acc] 96.88 [G Loss: 3.700874]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 337 [D Loss: 0.077019, acc] 99.22 [G Loss: 3.806108]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 338 [D Loss: 0.084891, acc] 98.44 [G Loss: 3.802259]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 339 [D Loss: 0.104492, acc] 96.09 [G Loss: 3.733651]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 340 [D Loss: 0.103565, acc] 97.66 [G Loss: 3.301760]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 341 [D Loss: 0.109718, acc] 96.88 [G Loss: 3.291399]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 342 [D Loss: 0.103333, acc] 96.88 [G Loss: 3.454558]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 343 [D Loss: 0.079093, acc] 98.44 [G Loss: 3.590269]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 344 [D Loss: 0.094128, acc] 96.88 [G Loss: 3.399055]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 345 [D Loss: 0.128080, acc] 94.53 [G Loss: 3.363062]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 346 [D Loss: 0.139330, acc] 96.09 [G Loss: 3.413308]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 347 [D Loss: 0.110869, acc] 95.31 [G Loss: 3.476840]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 348 [D Loss: 0.078637, acc] 98.44 [G Loss: 3.701925]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 349 [D Loss: 0.135232, acc] 96.09 [G Loss: 3.468635]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 350 [D Loss: 0.092739, acc] 98.44 [G Loss: 3.118182]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 351 [D Loss: 0.099662, acc] 97.66 [G Loss: 3.346845]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 352 [D Loss: 0.117301, acc] 95.31 [G Loss: 3.802336]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 353 [D Loss: 0.089465, acc] 98.44 [G Loss: 3.661883]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 354 [D Loss: 0.099302, acc] 96.88 [G Loss: 3.799356]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 355 [D Loss: 0.127834, acc] 96.09 [G Loss: 3.671655]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 356 [D Loss: 0.075109, acc] 98.44 [G Loss: 3.245600]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 357 [D Loss: 0.089950, acc] 98.44 [G Loss: 3.458908]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 358 [D Loss: 0.089790, acc] 97.66 [G Loss: 4.191590]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 359 [D Loss: 0.087249, acc] 96.88 [G Loss: 3.786810]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 360 [D Loss: 0.087644, acc] 97.66 [G Loss: 3.786380]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 361 [D Loss: 0.103838, acc] 96.88 [G Loss: 3.542411]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 362 [D Loss: 0.108552, acc] 96.09 [G Loss: 3.608192]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 363 [D Loss: 0.072168, acc] 97.66 [G Loss: 3.901989]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 364 [D Loss: 0.074797, acc] 98.44 [G Loss: 4.223248]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 365 [D Loss: 0.058673, acc] 99.22 [G Loss: 3.621205]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 366 [D Loss: 0.078019, acc] 97.66 [G Loss: 3.603776]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 367 [D Loss: 0.070919, acc] 99.22 [G Loss: 3.617399]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 368 [D Loss: 0.046884, acc] 99.22 [G Loss: 3.717301]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 369 [D Loss: 0.076548, acc] 98.44 [G Loss: 3.336924]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 370 [D Loss: 0.097799, acc] 97.66 [G Loss: 3.720288]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 371 [D Loss: 0.073999, acc] 98.44 [G Loss: 3.765193]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 372 [D Loss: 0.099174, acc] 96.88 [G Loss: 3.603572]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 373 [D Loss: 0.060808, acc] 100.00 [G Loss: 3.568186]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 374 [D Loss: 0.098934, acc] 95.31 [G Loss: 3.739305]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 375 [D Loss: 0.087448, acc] 98.44 [G Loss: 3.951015]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 376 [D Loss: 0.056325, acc] 99.22 [G Loss: 4.158186]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 377 [D Loss: 0.094340, acc] 97.66 [G Loss: 4.145516]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 378 [D Loss: 0.094884, acc] 97.66 [G Loss: 3.945095]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 379 [D Loss: 0.060011, acc] 99.22 [G Loss: 4.017280]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 380 [D Loss: 0.078402, acc] 97.66 [G Loss: 3.919580]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 381 [D Loss: 0.041430, acc] 100.00 [G Loss: 3.869562]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 382 [D Loss: 0.054300, acc] 100.00 [G Loss: 3.623292]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 383 [D Loss: 0.068116, acc] 99.22 [G Loss: 3.617959]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 384 [D Loss: 0.046291, acc] 100.00 [G Loss: 3.598804]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 385 [D Loss: 0.070061, acc] 99.22 [G Loss: 3.707661]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 386 [D Loss: 0.052522, acc] 100.00 [G Loss: 3.708520]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 387 [D Loss: 0.072063, acc] 99.22 [G Loss: 3.859343]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 388 [D Loss: 0.065148, acc] 99.22 [G Loss: 3.892299]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 389 [D Loss: 0.065669, acc] 100.00 [G Loss: 3.914027]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 390 [D Loss: 0.058234, acc] 99.22 [G Loss: 4.234095]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 391 [D Loss: 0.072074, acc] 98.44 [G Loss: 3.881571]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 392 [D Loss: 0.117037, acc] 95.31 [G Loss: 3.909061]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 393 [D Loss: 0.055163, acc] 98.44 [G Loss: 4.274844]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 394 [D Loss: 0.066229, acc] 100.00 [G Loss: 4.070475]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 395 [D Loss: 0.077283, acc] 99.22 [G Loss: 4.179668]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 396 [D Loss: 0.075387, acc] 98.44 [G Loss: 3.993660]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 397 [D Loss: 0.056653, acc] 99.22 [G Loss: 4.115586]\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "************ 398 [D Loss: 0.038465, acc] 100.00 [G Loss: 4.301497]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 399 [D Loss: 0.049290, acc] 99.22 [G Loss: 4.042812]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 400 [D Loss: 0.044022, acc] 99.22 [G Loss: 4.059758]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.00000003\n",
            "saved\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 401 [D Loss: 0.055505, acc] 100.00 [G Loss: 4.040528]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 402 [D Loss: 0.045888, acc] 100.00 [G Loss: 4.184083]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 403 [D Loss: 0.041570, acc] 100.00 [G Loss: 3.963940]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 404 [D Loss: 0.042258, acc] 100.00 [G Loss: 3.797143]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 405 [D Loss: 0.091111, acc] 96.88 [G Loss: 4.375494]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 406 [D Loss: 0.056634, acc] 99.22 [G Loss: 4.567447]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 407 [D Loss: 0.056869, acc] 99.22 [G Loss: 4.610237]\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "************ 408 [D Loss: 0.045969, acc] 98.44 [G Loss: 4.295319]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 409 [D Loss: 0.047750, acc] 99.22 [G Loss: 3.991476]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 410 [D Loss: 0.065135, acc] 99.22 [G Loss: 4.130103]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 411 [D Loss: 0.073491, acc] 96.88 [G Loss: 4.372893]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 412 [D Loss: 0.055949, acc] 100.00 [G Loss: 4.406643]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 413 [D Loss: 0.052496, acc] 100.00 [G Loss: 4.882468]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 414 [D Loss: 0.056217, acc] 99.22 [G Loss: 4.449790]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 415 [D Loss: 0.067296, acc] 99.22 [G Loss: 4.101537]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 416 [D Loss: 0.046121, acc] 100.00 [G Loss: 4.499507]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 417 [D Loss: 0.072223, acc] 99.22 [G Loss: 4.533909]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 418 [D Loss: 0.036460, acc] 100.00 [G Loss: 4.423739]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 419 [D Loss: 0.035297, acc] 100.00 [G Loss: 5.122365]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 420 [D Loss: 0.032258, acc] 100.00 [G Loss: 4.816230]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 421 [D Loss: 0.050658, acc] 100.00 [G Loss: 4.586711]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 422 [D Loss: 0.039078, acc] 100.00 [G Loss: 4.402741]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 423 [D Loss: 0.060731, acc] 99.22 [G Loss: 4.143017]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 424 [D Loss: 0.042439, acc] 100.00 [G Loss: 4.343544]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 425 [D Loss: 0.043260, acc] 100.00 [G Loss: 4.415450]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 426 [D Loss: 0.031867, acc] 100.00 [G Loss: 4.537971]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 427 [D Loss: 0.052389, acc] 100.00 [G Loss: 4.230779]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 428 [D Loss: 0.051553, acc] 100.00 [G Loss: 4.447302]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 429 [D Loss: 0.046822, acc] 99.22 [G Loss: 4.513424]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 430 [D Loss: 0.046883, acc] 99.22 [G Loss: 4.244152]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 431 [D Loss: 0.035808, acc] 100.00 [G Loss: 4.406083]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 432 [D Loss: 0.037045, acc] 100.00 [G Loss: 4.201834]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 433 [D Loss: 0.028446, acc] 100.00 [G Loss: 4.428765]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 434 [D Loss: 0.041628, acc] 100.00 [G Loss: 4.498562]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 435 [D Loss: 0.052013, acc] 99.22 [G Loss: 4.428101]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 436 [D Loss: 0.054685, acc] 99.22 [G Loss: 3.829202]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 437 [D Loss: 0.038320, acc] 100.00 [G Loss: 3.967845]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 438 [D Loss: 0.051817, acc] 99.22 [G Loss: 4.123594]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 439 [D Loss: 0.045458, acc] 100.00 [G Loss: 4.415436]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 440 [D Loss: 0.069182, acc] 99.22 [G Loss: 4.600512]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 441 [D Loss: 0.051412, acc] 98.44 [G Loss: 4.751812]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 442 [D Loss: 0.065151, acc] 99.22 [G Loss: 4.522376]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 443 [D Loss: 0.102446, acc] 97.66 [G Loss: 4.198907]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 444 [D Loss: 0.047952, acc] 100.00 [G Loss: 4.755929]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 445 [D Loss: 0.044134, acc] 100.00 [G Loss: 4.698157]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 446 [D Loss: 0.025601, acc] 100.00 [G Loss: 4.823037]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 447 [D Loss: 0.047685, acc] 99.22 [G Loss: 4.545257]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 448 [D Loss: 0.039252, acc] 99.22 [G Loss: 4.408898]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 449 [D Loss: 0.053689, acc] 100.00 [G Loss: 4.407556]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 450 [D Loss: 0.039167, acc] 100.00 [G Loss: 4.766755]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 451 [D Loss: 0.033818, acc] 99.22 [G Loss: 4.511716]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 452 [D Loss: 0.045244, acc] 99.22 [G Loss: 4.478680]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 453 [D Loss: 0.031638, acc] 100.00 [G Loss: 4.311429]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 454 [D Loss: 0.040070, acc] 100.00 [G Loss: 4.465010]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 455 [D Loss: 0.039130, acc] 99.22 [G Loss: 4.603627]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 456 [D Loss: 0.062129, acc] 99.22 [G Loss: 4.562827]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 457 [D Loss: 0.052040, acc] 100.00 [G Loss: 4.434595]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 458 [D Loss: 0.047462, acc] 99.22 [G Loss: 4.556542]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 459 [D Loss: 0.071890, acc] 97.66 [G Loss: 4.727791]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 460 [D Loss: 0.046310, acc] 99.22 [G Loss: 4.986528]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 461 [D Loss: 0.043596, acc] 99.22 [G Loss: 4.951950]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 462 [D Loss: 0.052449, acc] 100.00 [G Loss: 4.855956]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 463 [D Loss: 0.049757, acc] 100.00 [G Loss: 4.819366]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 464 [D Loss: 0.045998, acc] 100.00 [G Loss: 4.689489]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 465 [D Loss: 0.048321, acc] 98.44 [G Loss: 4.733301]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 466 [D Loss: 0.033821, acc] 100.00 [G Loss: 5.088390]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 467 [D Loss: 0.041276, acc] 99.22 [G Loss: 5.056094]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 468 [D Loss: 0.040534, acc] 99.22 [G Loss: 4.514730]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 469 [D Loss: 0.067555, acc] 99.22 [G Loss: 4.249067]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 470 [D Loss: 0.083784, acc] 98.44 [G Loss: 4.496243]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 471 [D Loss: 0.034934, acc] 99.22 [G Loss: 4.306641]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 472 [D Loss: 0.061600, acc] 99.22 [G Loss: 4.365379]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 473 [D Loss: 0.062742, acc] 100.00 [G Loss: 4.555883]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 474 [D Loss: 0.043922, acc] 99.22 [G Loss: 4.857358]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 475 [D Loss: 0.062591, acc] 98.44 [G Loss: 4.224200]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 476 [D Loss: 0.055295, acc] 99.22 [G Loss: 4.207491]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 477 [D Loss: 0.039284, acc] 100.00 [G Loss: 4.642420]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 478 [D Loss: 0.036413, acc] 100.00 [G Loss: 5.048834]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 479 [D Loss: 0.046881, acc] 99.22 [G Loss: 4.813845]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 480 [D Loss: 0.049677, acc] 100.00 [G Loss: 4.679914]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 481 [D Loss: 0.025177, acc] 100.00 [G Loss: 4.378849]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 482 [D Loss: 0.064184, acc] 99.22 [G Loss: 4.312659]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 483 [D Loss: 0.040808, acc] 100.00 [G Loss: 4.993660]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 484 [D Loss: 0.043765, acc] 98.44 [G Loss: 4.659140]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 485 [D Loss: 0.060511, acc] 98.44 [G Loss: 4.291855]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 486 [D Loss: 0.051522, acc] 99.22 [G Loss: 4.719072]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 487 [D Loss: 0.091046, acc] 96.88 [G Loss: 4.602222]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 488 [D Loss: 0.046674, acc] 100.00 [G Loss: 5.048904]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 489 [D Loss: 0.035325, acc] 100.00 [G Loss: 5.215600]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 490 [D Loss: 0.040759, acc] 99.22 [G Loss: 4.847786]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 491 [D Loss: 0.062113, acc] 98.44 [G Loss: 4.903845]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 492 [D Loss: 0.048972, acc] 98.44 [G Loss: 4.742315]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 493 [D Loss: 0.056755, acc] 98.44 [G Loss: 4.810582]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 494 [D Loss: 0.042075, acc] 100.00 [G Loss: 5.408157]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 495 [D Loss: 0.027449, acc] 100.00 [G Loss: 5.187747]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 496 [D Loss: 0.037398, acc] 100.00 [G Loss: 5.064147]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 497 [D Loss: 0.037106, acc] 100.00 [G Loss: 4.858928]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 498 [D Loss: 0.044017, acc] 100.00 [G Loss: 3.895592]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 499 [D Loss: 0.072771, acc] 97.66 [G Loss: 4.316460]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 500 [D Loss: 0.046554, acc] 100.00 [G Loss: 5.118509]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 501 [D Loss: 0.064581, acc] 99.22 [G Loss: 4.881505]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 502 [D Loss: 0.047023, acc] 100.00 [G Loss: 4.614261]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 503 [D Loss: 0.067226, acc] 98.44 [G Loss: 4.370454]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 504 [D Loss: 0.075294, acc] 97.66 [G Loss: 4.200948]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 505 [D Loss: 0.051401, acc] 99.22 [G Loss: 4.881963]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 506 [D Loss: 0.025161, acc] 100.00 [G Loss: 4.857202]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 507 [D Loss: 0.038068, acc] 100.00 [G Loss: 5.036204]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 508 [D Loss: 0.037189, acc] 99.22 [G Loss: 4.713897]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 509 [D Loss: 0.058790, acc] 98.44 [G Loss: 4.572057]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 510 [D Loss: 0.044674, acc] 99.22 [G Loss: 4.419911]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 511 [D Loss: 0.074895, acc] 97.66 [G Loss: 4.586731]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 512 [D Loss: 0.030865, acc] 100.00 [G Loss: 4.621118]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 513 [D Loss: 0.039417, acc] 100.00 [G Loss: 4.403234]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 514 [D Loss: 0.036595, acc] 99.22 [G Loss: 4.543456]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 515 [D Loss: 0.067894, acc] 98.44 [G Loss: 4.312242]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 516 [D Loss: 0.047059, acc] 99.22 [G Loss: 4.531662]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 517 [D Loss: 0.043053, acc] 100.00 [G Loss: 4.552439]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 518 [D Loss: 0.059719, acc] 99.22 [G Loss: 4.141409]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 519 [D Loss: 0.061685, acc] 99.22 [G Loss: 3.762868]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 520 [D Loss: 0.084344, acc] 97.66 [G Loss: 4.345718]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 521 [D Loss: 0.035844, acc] 100.00 [G Loss: 4.758713]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 522 [D Loss: 0.088857, acc] 98.44 [G Loss: 4.863240]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 523 [D Loss: 0.047959, acc] 100.00 [G Loss: 4.790918]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 524 [D Loss: 0.073335, acc] 99.22 [G Loss: 4.214722]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 525 [D Loss: 0.055183, acc] 100.00 [G Loss: 4.079961]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 526 [D Loss: 0.051691, acc] 99.22 [G Loss: 4.546727]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 527 [D Loss: 0.035837, acc] 100.00 [G Loss: 5.308738]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 528 [D Loss: 0.040176, acc] 99.22 [G Loss: 5.203384]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 529 [D Loss: 0.062609, acc] 99.22 [G Loss: 4.598595]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 530 [D Loss: 0.039398, acc] 99.22 [G Loss: 4.424803]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 531 [D Loss: 0.091017, acc] 98.44 [G Loss: 4.350465]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 532 [D Loss: 0.035942, acc] 99.22 [G Loss: 4.320668]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 533 [D Loss: 0.040870, acc] 99.22 [G Loss: 4.846782]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 534 [D Loss: 0.058238, acc] 98.44 [G Loss: 4.735849]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 535 [D Loss: 0.033985, acc] 100.00 [G Loss: 4.754605]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 536 [D Loss: 0.040808, acc] 99.22 [G Loss: 4.772601]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 537 [D Loss: 0.038186, acc] 99.22 [G Loss: 4.558472]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 538 [D Loss: 0.031357, acc] 100.00 [G Loss: 4.467945]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 539 [D Loss: 0.041388, acc] 99.22 [G Loss: 4.701435]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 540 [D Loss: 0.028222, acc] 99.22 [G Loss: 4.501132]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 541 [D Loss: 0.037982, acc] 100.00 [G Loss: 4.436939]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 542 [D Loss: 0.052735, acc] 98.44 [G Loss: 4.496310]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 543 [D Loss: 0.102046, acc] 96.09 [G Loss: 4.508421]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 544 [D Loss: 0.067029, acc] 96.88 [G Loss: 4.865823]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 545 [D Loss: 0.075075, acc] 98.44 [G Loss: 4.895914]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 546 [D Loss: 0.061978, acc] 99.22 [G Loss: 4.985255]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 547 [D Loss: 0.042279, acc] 100.00 [G Loss: 4.920442]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 548 [D Loss: 0.044714, acc] 100.00 [G Loss: 4.682312]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 549 [D Loss: 0.030804, acc] 100.00 [G Loss: 5.223343]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 550 [D Loss: 0.048687, acc] 99.22 [G Loss: 5.223367]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 551 [D Loss: 0.029193, acc] 100.00 [G Loss: 4.992608]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 552 [D Loss: 0.062935, acc] 96.88 [G Loss: 4.380519]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 553 [D Loss: 0.044473, acc] 99.22 [G Loss: 4.433496]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 554 [D Loss: 0.050108, acc] 97.66 [G Loss: 4.845016]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 555 [D Loss: 0.029074, acc] 100.00 [G Loss: 4.864301]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 556 [D Loss: 0.051298, acc] 98.44 [G Loss: 4.724739]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 557 [D Loss: 0.039470, acc] 98.44 [G Loss: 4.516481]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 558 [D Loss: 0.054202, acc] 98.44 [G Loss: 4.527340]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 559 [D Loss: 0.036345, acc] 100.00 [G Loss: 4.633370]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 560 [D Loss: 0.064341, acc] 97.66 [G Loss: 4.267761]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 561 [D Loss: 0.058456, acc] 99.22 [G Loss: 4.434932]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 562 [D Loss: 0.055855, acc] 98.44 [G Loss: 5.018394]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 563 [D Loss: 0.071890, acc] 98.44 [G Loss: 4.334867]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 564 [D Loss: 0.059814, acc] 100.00 [G Loss: 4.315226]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 565 [D Loss: 0.055730, acc] 99.22 [G Loss: 4.861732]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 566 [D Loss: 0.041419, acc] 100.00 [G Loss: 5.344970]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 567 [D Loss: 0.046134, acc] 98.44 [G Loss: 5.573057]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 568 [D Loss: 0.111823, acc] 96.09 [G Loss: 4.815824]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 569 [D Loss: 0.088256, acc] 97.66 [G Loss: 4.664800]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 570 [D Loss: 0.029450, acc] 100.00 [G Loss: 4.623058]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 571 [D Loss: 0.021794, acc] 100.00 [G Loss: 5.838777]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 572 [D Loss: 0.038452, acc] 100.00 [G Loss: 5.549645]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 573 [D Loss: 0.061893, acc] 99.22 [G Loss: 4.846465]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 574 [D Loss: 0.053782, acc] 98.44 [G Loss: 4.308420]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 575 [D Loss: 0.036835, acc] 99.22 [G Loss: 4.739269]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 576 [D Loss: 0.034526, acc] 100.00 [G Loss: 5.071366]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 577 [D Loss: 0.071330, acc] 98.44 [G Loss: 4.837307]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 578 [D Loss: 0.040195, acc] 100.00 [G Loss: 4.685919]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 579 [D Loss: 0.138253, acc] 94.53 [G Loss: 4.510860]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 580 [D Loss: 0.078548, acc] 96.88 [G Loss: 5.074108]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 581 [D Loss: 0.118921, acc] 96.88 [G Loss: 5.011731]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 582 [D Loss: 0.053474, acc] 98.44 [G Loss: 4.908203]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 583 [D Loss: 0.035523, acc] 98.44 [G Loss: 4.561840]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 584 [D Loss: 0.049342, acc] 99.22 [G Loss: 4.143229]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 585 [D Loss: 0.050832, acc] 100.00 [G Loss: 4.400153]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 586 [D Loss: 0.037400, acc] 100.00 [G Loss: 4.466254]\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "************ 587 [D Loss: 0.066956, acc] 98.44 [G Loss: 4.595175]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 588 [D Loss: 0.049273, acc] 99.22 [G Loss: 4.749763]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 589 [D Loss: 0.057038, acc] 100.00 [G Loss: 4.849635]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 590 [D Loss: 0.039180, acc] 99.22 [G Loss: 5.041359]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 591 [D Loss: 0.028792, acc] 100.00 [G Loss: 5.226932]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 592 [D Loss: 0.051373, acc] 99.22 [G Loss: 5.208581]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 593 [D Loss: 0.040923, acc] 98.44 [G Loss: 4.638257]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 594 [D Loss: 0.070239, acc] 98.44 [G Loss: 4.631775]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 595 [D Loss: 0.046590, acc] 98.44 [G Loss: 5.078504]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 596 [D Loss: 0.040872, acc] 100.00 [G Loss: 5.213270]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 597 [D Loss: 0.034844, acc] 100.00 [G Loss: 5.322459]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 598 [D Loss: 0.027394, acc] 100.00 [G Loss: 5.167328]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 599 [D Loss: 0.048110, acc] 100.00 [G Loss: 4.197808]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 600 [D Loss: 0.060515, acc] 98.44 [G Loss: 4.566745]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "0.00000004\n",
            "saved\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 601 [D Loss: 0.048974, acc] 99.22 [G Loss: 5.310262]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 602 [D Loss: 0.035435, acc] 100.00 [G Loss: 5.264602]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 603 [D Loss: 0.069957, acc] 99.22 [G Loss: 4.992370]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 604 [D Loss: 0.034241, acc] 100.00 [G Loss: 4.139946]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 605 [D Loss: 0.081693, acc] 97.66 [G Loss: 3.951667]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 606 [D Loss: 0.055291, acc] 99.22 [G Loss: 4.497256]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 607 [D Loss: 0.040083, acc] 100.00 [G Loss: 5.573008]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 608 [D Loss: 0.126992, acc] 94.53 [G Loss: 4.689243]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 609 [D Loss: 0.049425, acc] 100.00 [G Loss: 3.876253]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 610 [D Loss: 0.063530, acc] 98.44 [G Loss: 4.115602]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 611 [D Loss: 0.039398, acc] 100.00 [G Loss: 4.724510]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 612 [D Loss: 0.048789, acc] 99.22 [G Loss: 5.018086]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 613 [D Loss: 0.057308, acc] 98.44 [G Loss: 4.328341]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 614 [D Loss: 0.059156, acc] 98.44 [G Loss: 4.959739]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 615 [D Loss: 0.038886, acc] 100.00 [G Loss: 4.631221]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 616 [D Loss: 0.034818, acc] 100.00 [G Loss: 4.781462]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 617 [D Loss: 0.030915, acc] 100.00 [G Loss: 4.780163]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 618 [D Loss: 0.026153, acc] 100.00 [G Loss: 4.849011]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 619 [D Loss: 0.050997, acc] 99.22 [G Loss: 4.394672]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 620 [D Loss: 0.097394, acc] 97.66 [G Loss: 3.897285]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 621 [D Loss: 0.066681, acc] 98.44 [G Loss: 4.668469]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 622 [D Loss: 0.047672, acc] 100.00 [G Loss: 5.492043]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 623 [D Loss: 0.111405, acc] 94.53 [G Loss: 4.497458]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 624 [D Loss: 0.064373, acc] 100.00 [G Loss: 3.972859]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 625 [D Loss: 0.073839, acc] 96.88 [G Loss: 4.721313]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 626 [D Loss: 0.037697, acc] 99.22 [G Loss: 6.278337]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 627 [D Loss: 0.092721, acc] 97.66 [G Loss: 5.298803]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 628 [D Loss: 0.090265, acc] 97.66 [G Loss: 3.890027]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 629 [D Loss: 0.100058, acc] 93.75 [G Loss: 4.340565]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 630 [D Loss: 0.038648, acc] 99.22 [G Loss: 5.177958]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 631 [D Loss: 0.045699, acc] 99.22 [G Loss: 5.532611]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 632 [D Loss: 0.073764, acc] 97.66 [G Loss: 4.398068]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 633 [D Loss: 0.066831, acc] 98.44 [G Loss: 4.112592]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 634 [D Loss: 0.047715, acc] 98.44 [G Loss: 4.346634]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 635 [D Loss: 0.046406, acc] 98.44 [G Loss: 5.168457]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 636 [D Loss: 0.049819, acc] 98.44 [G Loss: 5.511950]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 637 [D Loss: 0.046207, acc] 99.22 [G Loss: 4.810199]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 638 [D Loss: 0.074086, acc] 99.22 [G Loss: 4.579481]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 639 [D Loss: 0.078386, acc] 97.66 [G Loss: 4.787186]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 640 [D Loss: 0.048808, acc] 100.00 [G Loss: 4.416252]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 641 [D Loss: 0.050125, acc] 98.44 [G Loss: 5.048362]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 642 [D Loss: 0.071265, acc] 99.22 [G Loss: 5.154760]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 643 [D Loss: 0.046035, acc] 99.22 [G Loss: 4.618973]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 644 [D Loss: 0.081543, acc] 96.88 [G Loss: 4.290344]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 645 [D Loss: 0.072660, acc] 98.44 [G Loss: 4.666137]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 646 [D Loss: 0.061250, acc] 99.22 [G Loss: 4.762103]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 647 [D Loss: 0.041313, acc] 100.00 [G Loss: 4.753327]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 648 [D Loss: 0.092030, acc] 98.44 [G Loss: 4.839201]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 649 [D Loss: 0.088669, acc] 97.66 [G Loss: 4.206607]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 650 [D Loss: 0.080934, acc] 98.44 [G Loss: 3.568204]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 651 [D Loss: 0.086184, acc] 96.88 [G Loss: 5.776958]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 652 [D Loss: 0.038527, acc] 100.00 [G Loss: 6.475324]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 653 [D Loss: 0.089863, acc] 96.09 [G Loss: 5.066802]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 654 [D Loss: 0.090391, acc] 97.66 [G Loss: 4.278142]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 655 [D Loss: 0.065424, acc] 99.22 [G Loss: 4.828456]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 656 [D Loss: 0.044705, acc] 99.22 [G Loss: 5.006626]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 657 [D Loss: 0.053324, acc] 99.22 [G Loss: 5.082981]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 658 [D Loss: 0.066424, acc] 98.44 [G Loss: 4.861777]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 659 [D Loss: 0.058027, acc] 99.22 [G Loss: 4.541529]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 660 [D Loss: 0.070177, acc] 96.88 [G Loss: 4.627515]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 661 [D Loss: 0.035451, acc] 100.00 [G Loss: 5.947027]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 662 [D Loss: 0.115162, acc] 96.09 [G Loss: 5.053966]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 663 [D Loss: 0.082631, acc] 96.88 [G Loss: 4.390629]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 664 [D Loss: 0.080677, acc] 97.66 [G Loss: 3.936840]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 665 [D Loss: 0.050266, acc] 99.22 [G Loss: 4.857826]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 666 [D Loss: 0.058326, acc] 98.44 [G Loss: 5.130216]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 667 [D Loss: 0.072387, acc] 97.66 [G Loss: 5.123806]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 668 [D Loss: 0.099744, acc] 98.44 [G Loss: 4.184537]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 669 [D Loss: 0.079222, acc] 98.44 [G Loss: 4.282446]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 670 [D Loss: 0.089562, acc] 96.09 [G Loss: 4.733378]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 671 [D Loss: 0.057914, acc] 99.22 [G Loss: 4.546127]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 672 [D Loss: 0.091469, acc] 96.88 [G Loss: 4.654448]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 673 [D Loss: 0.058890, acc] 98.44 [G Loss: 4.773363]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 674 [D Loss: 0.094427, acc] 97.66 [G Loss: 5.209163]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 675 [D Loss: 0.058337, acc] 98.44 [G Loss: 4.686794]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 676 [D Loss: 0.067424, acc] 99.22 [G Loss: 4.837928]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 677 [D Loss: 0.059578, acc] 98.44 [G Loss: 4.244791]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 678 [D Loss: 0.063973, acc] 99.22 [G Loss: 4.931692]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 679 [D Loss: 0.138830, acc] 94.53 [G Loss: 4.760201]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 680 [D Loss: 0.063127, acc] 97.66 [G Loss: 4.488068]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 681 [D Loss: 0.086783, acc] 96.09 [G Loss: 4.092389]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 682 [D Loss: 0.063354, acc] 99.22 [G Loss: 3.958655]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 683 [D Loss: 0.050426, acc] 98.44 [G Loss: 5.056544]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 684 [D Loss: 0.079135, acc] 96.09 [G Loss: 4.990902]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 685 [D Loss: 0.196760, acc] 94.53 [G Loss: 4.246661]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 686 [D Loss: 0.097470, acc] 96.09 [G Loss: 3.921898]\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "************ 687 [D Loss: 0.057349, acc] 100.00 [G Loss: 4.359923]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 688 [D Loss: 0.093041, acc] 96.88 [G Loss: 5.159276]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 689 [D Loss: 0.120520, acc] 95.31 [G Loss: 4.101685]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 690 [D Loss: 0.068080, acc] 98.44 [G Loss: 3.437345]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 691 [D Loss: 0.081202, acc] 96.09 [G Loss: 4.702768]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 692 [D Loss: 0.089685, acc] 96.88 [G Loss: 5.147229]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 693 [D Loss: 0.072371, acc] 96.88 [G Loss: 4.726417]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 694 [D Loss: 0.067064, acc] 98.44 [G Loss: 4.262613]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 695 [D Loss: 0.076662, acc] 98.44 [G Loss: 4.505185]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 696 [D Loss: 0.058384, acc] 99.22 [G Loss: 5.061470]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 697 [D Loss: 0.105097, acc] 96.09 [G Loss: 4.467542]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 698 [D Loss: 0.080732, acc] 97.66 [G Loss: 4.009079]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 699 [D Loss: 0.060823, acc] 98.44 [G Loss: 4.502686]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 700 [D Loss: 0.106177, acc] 98.44 [G Loss: 4.697828]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 701 [D Loss: 0.100446, acc] 96.88 [G Loss: 4.404624]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 702 [D Loss: 0.093101, acc] 96.88 [G Loss: 4.592937]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 703 [D Loss: 0.075197, acc] 97.66 [G Loss: 4.948727]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 704 [D Loss: 0.135905, acc] 95.31 [G Loss: 4.393788]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 705 [D Loss: 0.099894, acc] 96.88 [G Loss: 4.148277]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 706 [D Loss: 0.172246, acc] 93.75 [G Loss: 3.772305]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 707 [D Loss: 0.097295, acc] 98.44 [G Loss: 5.059900]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 708 [D Loss: 0.135489, acc] 95.31 [G Loss: 5.359661]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 709 [D Loss: 0.055979, acc] 100.00 [G Loss: 5.606607]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 710 [D Loss: 0.150557, acc] 93.75 [G Loss: 4.466094]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 711 [D Loss: 0.106779, acc] 96.88 [G Loss: 4.239713]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 712 [D Loss: 0.045316, acc] 100.00 [G Loss: 4.879398]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 713 [D Loss: 0.069402, acc] 99.22 [G Loss: 4.564292]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 714 [D Loss: 0.091283, acc] 98.44 [G Loss: 4.564034]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 715 [D Loss: 0.081190, acc] 98.44 [G Loss: 4.679213]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 716 [D Loss: 0.060395, acc] 99.22 [G Loss: 4.466343]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 717 [D Loss: 0.131042, acc] 97.66 [G Loss: 4.346045]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 718 [D Loss: 0.074370, acc] 98.44 [G Loss: 3.976207]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 719 [D Loss: 0.058368, acc] 100.00 [G Loss: 4.330970]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 720 [D Loss: 0.062752, acc] 99.22 [G Loss: 5.318031]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 721 [D Loss: 0.084747, acc] 98.44 [G Loss: 4.286759]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 722 [D Loss: 0.184823, acc] 93.75 [G Loss: 4.260369]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 723 [D Loss: 0.106572, acc] 96.09 [G Loss: 5.397969]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 724 [D Loss: 0.089496, acc] 97.66 [G Loss: 5.576062]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 725 [D Loss: 0.090517, acc] 96.88 [G Loss: 4.484400]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 726 [D Loss: 0.112912, acc] 96.09 [G Loss: 4.650219]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 727 [D Loss: 0.052174, acc] 99.22 [G Loss: 5.229853]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 728 [D Loss: 0.064166, acc] 99.22 [G Loss: 4.703595]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 729 [D Loss: 0.136076, acc] 96.88 [G Loss: 3.869061]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 730 [D Loss: 0.114326, acc] 94.53 [G Loss: 4.436451]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 731 [D Loss: 0.040761, acc] 100.00 [G Loss: 5.760912]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 732 [D Loss: 0.213219, acc] 91.41 [G Loss: 4.090725]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 733 [D Loss: 0.141583, acc] 92.97 [G Loss: 4.365839]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 734 [D Loss: 0.069379, acc] 99.22 [G Loss: 4.772681]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 735 [D Loss: 0.107834, acc] 95.31 [G Loss: 4.708359]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 736 [D Loss: 0.112379, acc] 97.66 [G Loss: 3.979105]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 737 [D Loss: 0.071554, acc] 97.66 [G Loss: 4.119958]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 738 [D Loss: 0.059067, acc] 100.00 [G Loss: 5.348425]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 739 [D Loss: 0.118024, acc] 96.88 [G Loss: 4.261117]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 740 [D Loss: 0.137206, acc] 95.31 [G Loss: 4.008863]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 741 [D Loss: 0.114534, acc] 96.09 [G Loss: 4.383440]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 742 [D Loss: 0.117805, acc] 96.88 [G Loss: 3.835882]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 743 [D Loss: 0.093845, acc] 96.09 [G Loss: 4.467665]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 744 [D Loss: 0.091647, acc] 98.44 [G Loss: 4.482476]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 745 [D Loss: 0.118498, acc] 93.75 [G Loss: 3.883068]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 746 [D Loss: 0.080244, acc] 100.00 [G Loss: 3.796960]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 747 [D Loss: 0.093686, acc] 96.88 [G Loss: 4.424772]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 748 [D Loss: 0.111457, acc] 96.09 [G Loss: 4.722428]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 749 [D Loss: 0.062877, acc] 98.44 [G Loss: 5.293010]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 750 [D Loss: 0.058230, acc] 98.44 [G Loss: 5.243799]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 751 [D Loss: 0.083841, acc] 96.09 [G Loss: 4.825603]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 752 [D Loss: 0.101057, acc] 96.88 [G Loss: 5.207057]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 753 [D Loss: 0.146810, acc] 95.31 [G Loss: 5.473470]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 754 [D Loss: 0.117976, acc] 96.09 [G Loss: 5.200269]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 755 [D Loss: 0.178733, acc] 93.75 [G Loss: 4.221638]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 756 [D Loss: 0.110432, acc] 96.09 [G Loss: 4.643270]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 757 [D Loss: 0.117299, acc] 97.66 [G Loss: 4.275688]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 758 [D Loss: 0.083997, acc] 98.44 [G Loss: 4.661703]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 759 [D Loss: 0.128920, acc] 95.31 [G Loss: 4.046087]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 760 [D Loss: 0.090558, acc] 97.66 [G Loss: 4.552394]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 761 [D Loss: 0.198526, acc] 92.19 [G Loss: 4.836453]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 762 [D Loss: 0.100252, acc] 96.88 [G Loss: 4.672197]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 763 [D Loss: 0.193071, acc] 92.19 [G Loss: 3.656897]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 764 [D Loss: 0.140504, acc] 94.53 [G Loss: 3.881895]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 765 [D Loss: 0.066567, acc] 97.66 [G Loss: 5.433344]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 766 [D Loss: 0.250622, acc] 92.97 [G Loss: 4.320868]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 767 [D Loss: 0.176316, acc] 90.62 [G Loss: 3.514074]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 768 [D Loss: 0.138129, acc] 94.53 [G Loss: 4.482976]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 769 [D Loss: 0.094200, acc] 98.44 [G Loss: 5.422039]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 770 [D Loss: 0.174591, acc] 94.53 [G Loss: 3.739417]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 771 [D Loss: 0.178054, acc] 92.97 [G Loss: 3.859759]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 772 [D Loss: 0.064092, acc] 99.22 [G Loss: 5.335361]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 773 [D Loss: 0.147380, acc] 92.97 [G Loss: 4.239863]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 774 [D Loss: 0.166836, acc] 94.53 [G Loss: 4.131277]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 775 [D Loss: 0.054179, acc] 99.22 [G Loss: 4.099007]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 776 [D Loss: 0.111740, acc] 96.09 [G Loss: 4.239550]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 777 [D Loss: 0.178905, acc] 93.75 [G Loss: 4.037602]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 778 [D Loss: 0.073829, acc] 98.44 [G Loss: 4.627888]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 779 [D Loss: 0.209691, acc] 90.62 [G Loss: 3.850558]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 780 [D Loss: 0.125182, acc] 95.31 [G Loss: 4.140537]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 781 [D Loss: 0.146420, acc] 96.09 [G Loss: 4.622648]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 782 [D Loss: 0.111213, acc] 97.66 [G Loss: 4.921911]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 783 [D Loss: 0.165241, acc] 95.31 [G Loss: 4.618800]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 784 [D Loss: 0.130898, acc] 95.31 [G Loss: 4.009889]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 785 [D Loss: 0.161438, acc] 93.75 [G Loss: 3.951004]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 786 [D Loss: 0.136481, acc] 96.09 [G Loss: 5.477495]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 787 [D Loss: 0.121596, acc] 97.66 [G Loss: 5.634465]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 788 [D Loss: 0.192205, acc] 92.19 [G Loss: 3.995384]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 789 [D Loss: 0.182811, acc] 92.19 [G Loss: 3.542950]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 790 [D Loss: 0.141478, acc] 94.53 [G Loss: 4.578582]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 791 [D Loss: 0.080109, acc] 99.22 [G Loss: 6.261074]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 792 [D Loss: 0.240984, acc] 91.41 [G Loss: 4.241962]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 793 [D Loss: 0.098400, acc] 96.88 [G Loss: 4.057916]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 794 [D Loss: 0.139864, acc] 93.75 [G Loss: 4.623909]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 795 [D Loss: 0.184436, acc] 92.97 [G Loss: 5.061553]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 796 [D Loss: 0.127358, acc] 96.09 [G Loss: 5.052762]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 797 [D Loss: 0.126972, acc] 96.88 [G Loss: 4.188724]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 798 [D Loss: 0.099430, acc] 96.88 [G Loss: 4.233547]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 799 [D Loss: 0.099742, acc] 96.88 [G Loss: 5.229157]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 800 [D Loss: 0.120568, acc] 96.88 [G Loss: 4.582334]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.00000005\n",
            "saved\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 801 [D Loss: 0.157159, acc] 94.53 [G Loss: 3.648356]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 802 [D Loss: 0.155625, acc] 95.31 [G Loss: 3.442278]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 803 [D Loss: 0.141925, acc] 95.31 [G Loss: 4.636539]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 804 [D Loss: 0.179933, acc] 92.19 [G Loss: 4.493445]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 805 [D Loss: 0.100282, acc] 96.09 [G Loss: 4.382067]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 806 [D Loss: 0.145574, acc] 94.53 [G Loss: 4.126368]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 807 [D Loss: 0.087309, acc] 97.66 [G Loss: 4.847881]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 808 [D Loss: 0.147826, acc] 94.53 [G Loss: 4.777188]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 809 [D Loss: 0.122184, acc] 96.88 [G Loss: 4.183391]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 810 [D Loss: 0.203815, acc] 90.62 [G Loss: 4.226576]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 811 [D Loss: 0.106583, acc] 98.44 [G Loss: 5.372118]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 812 [D Loss: 0.099021, acc] 96.88 [G Loss: 4.714210]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 813 [D Loss: 0.118986, acc] 96.88 [G Loss: 4.107834]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 814 [D Loss: 0.197395, acc] 94.53 [G Loss: 3.786392]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 815 [D Loss: 0.112678, acc] 98.44 [G Loss: 4.573276]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 816 [D Loss: 0.198751, acc] 93.75 [G Loss: 4.065387]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 817 [D Loss: 0.168522, acc] 93.75 [G Loss: 3.593031]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 818 [D Loss: 0.147358, acc] 94.53 [G Loss: 3.970741]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 819 [D Loss: 0.130944, acc] 94.53 [G Loss: 4.560480]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 820 [D Loss: 0.083719, acc] 97.66 [G Loss: 4.599708]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 821 [D Loss: 0.122223, acc] 97.66 [G Loss: 3.759820]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 822 [D Loss: 0.193182, acc] 92.97 [G Loss: 2.961460]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 823 [D Loss: 0.098056, acc] 96.88 [G Loss: 4.364304]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 824 [D Loss: 0.125203, acc] 96.88 [G Loss: 4.642292]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 825 [D Loss: 0.248240, acc] 91.41 [G Loss: 3.134993]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 826 [D Loss: 0.222864, acc] 92.97 [G Loss: 4.104876]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 827 [D Loss: 0.145628, acc] 94.53 [G Loss: 4.799607]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 828 [D Loss: 0.157277, acc] 92.97 [G Loss: 4.008391]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 829 [D Loss: 0.185023, acc] 96.88 [G Loss: 3.570340]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 830 [D Loss: 0.072179, acc] 98.44 [G Loss: 4.440876]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 831 [D Loss: 0.154785, acc] 94.53 [G Loss: 4.890849]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 832 [D Loss: 0.105613, acc] 97.66 [G Loss: 4.561395]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 833 [D Loss: 0.109928, acc] 96.09 [G Loss: 4.440111]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 834 [D Loss: 0.119322, acc] 96.09 [G Loss: 4.328169]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 835 [D Loss: 0.058294, acc] 98.44 [G Loss: 4.892648]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 836 [D Loss: 0.114143, acc] 96.88 [G Loss: 4.352330]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 837 [D Loss: 0.104154, acc] 96.09 [G Loss: 3.654701]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 838 [D Loss: 0.165820, acc] 95.31 [G Loss: 3.739913]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 839 [D Loss: 0.197849, acc] 89.84 [G Loss: 4.223971]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 840 [D Loss: 0.181083, acc] 91.41 [G Loss: 4.812689]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 841 [D Loss: 0.160983, acc] 92.97 [G Loss: 3.989866]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 842 [D Loss: 0.149788, acc] 95.31 [G Loss: 3.992750]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 843 [D Loss: 0.211271, acc] 91.41 [G Loss: 4.005292]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 844 [D Loss: 0.136540, acc] 95.31 [G Loss: 4.909053]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 845 [D Loss: 0.166573, acc] 92.97 [G Loss: 5.461819]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 846 [D Loss: 0.155836, acc] 95.31 [G Loss: 3.986990]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 847 [D Loss: 0.212718, acc] 92.97 [G Loss: 3.562198]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 848 [D Loss: 0.105403, acc] 95.31 [G Loss: 4.448168]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 849 [D Loss: 0.132314, acc] 94.53 [G Loss: 4.871142]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 850 [D Loss: 0.113597, acc] 96.88 [G Loss: 4.761956]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 851 [D Loss: 0.191466, acc] 92.97 [G Loss: 3.336256]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 852 [D Loss: 0.179501, acc] 92.19 [G Loss: 3.573257]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 853 [D Loss: 0.120110, acc] 95.31 [G Loss: 4.922214]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 854 [D Loss: 0.093672, acc] 99.22 [G Loss: 4.695213]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 855 [D Loss: 0.102403, acc] 99.22 [G Loss: 3.984512]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 856 [D Loss: 0.168503, acc] 92.97 [G Loss: 3.932036]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 857 [D Loss: 0.104010, acc] 97.66 [G Loss: 3.519327]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 858 [D Loss: 0.136734, acc] 94.53 [G Loss: 3.786421]\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "************ 859 [D Loss: 0.161037, acc] 93.75 [G Loss: 4.156707]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 860 [D Loss: 0.136858, acc] 94.53 [G Loss: 4.367152]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 861 [D Loss: 0.093707, acc] 96.88 [G Loss: 4.576902]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 862 [D Loss: 0.157469, acc] 94.53 [G Loss: 3.646631]\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "************ 863 [D Loss: 0.148941, acc] 94.53 [G Loss: 3.623919]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 864 [D Loss: 0.095779, acc] 99.22 [G Loss: 4.802280]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 865 [D Loss: 0.175962, acc] 94.53 [G Loss: 3.962809]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 866 [D Loss: 0.145179, acc] 94.53 [G Loss: 3.293147]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 867 [D Loss: 0.145860, acc] 93.75 [G Loss: 3.985463]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 868 [D Loss: 0.104987, acc] 97.66 [G Loss: 4.453371]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 869 [D Loss: 0.129258, acc] 96.09 [G Loss: 4.884018]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 870 [D Loss: 0.167849, acc] 95.31 [G Loss: 3.463037]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 871 [D Loss: 0.139828, acc] 94.53 [G Loss: 3.126913]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 872 [D Loss: 0.200259, acc] 92.19 [G Loss: 4.902034]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 873 [D Loss: 0.119696, acc] 96.88 [G Loss: 5.421780]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 874 [D Loss: 0.199973, acc] 93.75 [G Loss: 4.758298]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 875 [D Loss: 0.172137, acc] 94.53 [G Loss: 5.163622]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 876 [D Loss: 0.106743, acc] 96.88 [G Loss: 4.742164]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 877 [D Loss: 0.178095, acc] 92.19 [G Loss: 4.723106]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 878 [D Loss: 0.133771, acc] 95.31 [G Loss: 3.968925]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 879 [D Loss: 0.156416, acc] 92.19 [G Loss: 4.984464]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 880 [D Loss: 0.207015, acc] 92.97 [G Loss: 5.348571]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 881 [D Loss: 0.111053, acc] 98.44 [G Loss: 5.947067]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 882 [D Loss: 0.190429, acc] 92.97 [G Loss: 4.223593]\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "************ 883 [D Loss: 0.167732, acc] 93.75 [G Loss: 4.018576]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 884 [D Loss: 0.115957, acc] 96.09 [G Loss: 4.603174]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 885 [D Loss: 0.161061, acc] 95.31 [G Loss: 4.854755]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 886 [D Loss: 0.220723, acc] 89.84 [G Loss: 4.863313]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 887 [D Loss: 0.124300, acc] 96.88 [G Loss: 4.939394]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 888 [D Loss: 0.120125, acc] 96.88 [G Loss: 4.368537]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 889 [D Loss: 0.174824, acc] 92.19 [G Loss: 4.102941]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 890 [D Loss: 0.222733, acc] 89.84 [G Loss: 4.074812]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 891 [D Loss: 0.193040, acc] 92.19 [G Loss: 4.981045]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 892 [D Loss: 0.250090, acc] 89.84 [G Loss: 3.900620]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 893 [D Loss: 0.188036, acc] 92.97 [G Loss: 3.883631]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 894 [D Loss: 0.209416, acc] 91.41 [G Loss: 3.849905]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 895 [D Loss: 0.138437, acc] 96.88 [G Loss: 4.439309]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 896 [D Loss: 0.152399, acc] 92.97 [G Loss: 4.257466]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 897 [D Loss: 0.205348, acc] 90.62 [G Loss: 4.000226]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 898 [D Loss: 0.263005, acc] 88.28 [G Loss: 4.011285]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 899 [D Loss: 0.300699, acc] 84.38 [G Loss: 3.942820]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 900 [D Loss: 0.162846, acc] 95.31 [G Loss: 3.913949]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 901 [D Loss: 0.247477, acc] 86.72 [G Loss: 3.824417]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 902 [D Loss: 0.233734, acc] 89.06 [G Loss: 3.099915]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 903 [D Loss: 0.198909, acc] 90.62 [G Loss: 3.213752]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 904 [D Loss: 0.223855, acc] 89.84 [G Loss: 5.199711]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 905 [D Loss: 0.232372, acc] 95.31 [G Loss: 4.860181]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 906 [D Loss: 0.230302, acc] 89.06 [G Loss: 3.459570]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 907 [D Loss: 0.216779, acc] 91.41 [G Loss: 3.354300]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 908 [D Loss: 0.171446, acc] 92.97 [G Loss: 3.724020]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 909 [D Loss: 0.148617, acc] 95.31 [G Loss: 4.845166]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 910 [D Loss: 0.154990, acc] 96.88 [G Loss: 4.593436]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 911 [D Loss: 0.308687, acc] 88.28 [G Loss: 3.206878]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 912 [D Loss: 0.285540, acc] 86.72 [G Loss: 2.897165]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 913 [D Loss: 0.153609, acc] 93.75 [G Loss: 4.270595]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 914 [D Loss: 0.214892, acc] 91.41 [G Loss: 4.743956]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 915 [D Loss: 0.176846, acc] 92.97 [G Loss: 4.442643]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 916 [D Loss: 0.264931, acc] 87.50 [G Loss: 4.333480]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 917 [D Loss: 0.180202, acc] 90.62 [G Loss: 4.434275]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 918 [D Loss: 0.184347, acc] 93.75 [G Loss: 4.496852]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 919 [D Loss: 0.126594, acc] 97.66 [G Loss: 4.283483]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 920 [D Loss: 0.144345, acc] 94.53 [G Loss: 4.279320]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 921 [D Loss: 0.111273, acc] 96.88 [G Loss: 4.952958]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 922 [D Loss: 0.147760, acc] 93.75 [G Loss: 4.488586]\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "************ 923 [D Loss: 0.249198, acc] 86.72 [G Loss: 3.772761]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 924 [D Loss: 0.258097, acc] 89.84 [G Loss: 4.553013]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 925 [D Loss: 0.202389, acc] 90.62 [G Loss: 5.227550]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 926 [D Loss: 0.292557, acc] 82.03 [G Loss: 4.126573]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 927 [D Loss: 0.234029, acc] 89.84 [G Loss: 3.191949]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 928 [D Loss: 0.201222, acc] 89.84 [G Loss: 4.328927]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 929 [D Loss: 0.200792, acc] 91.41 [G Loss: 4.881340]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 930 [D Loss: 0.177276, acc] 92.97 [G Loss: 3.916214]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 931 [D Loss: 0.139297, acc] 95.31 [G Loss: 3.283627]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 932 [D Loss: 0.204441, acc] 92.19 [G Loss: 4.157998]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 933 [D Loss: 0.126410, acc] 96.09 [G Loss: 5.142589]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 934 [D Loss: 0.262091, acc] 89.84 [G Loss: 4.072023]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 935 [D Loss: 0.207020, acc] 89.84 [G Loss: 3.743712]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 936 [D Loss: 0.215858, acc] 90.62 [G Loss: 3.912420]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 937 [D Loss: 0.094641, acc] 97.66 [G Loss: 5.298538]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 938 [D Loss: 0.226376, acc] 92.97 [G Loss: 4.278916]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 939 [D Loss: 0.228330, acc] 92.19 [G Loss: 3.695593]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 940 [D Loss: 0.283195, acc] 86.72 [G Loss: 3.851519]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 941 [D Loss: 0.212573, acc] 92.97 [G Loss: 4.343117]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 942 [D Loss: 0.154892, acc] 93.75 [G Loss: 4.262934]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 943 [D Loss: 0.169291, acc] 95.31 [G Loss: 3.864797]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 944 [D Loss: 0.195294, acc] 92.19 [G Loss: 3.606795]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 945 [D Loss: 0.207546, acc] 92.97 [G Loss: 3.555663]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 946 [D Loss: 0.198409, acc] 90.62 [G Loss: 4.150497]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 947 [D Loss: 0.171390, acc] 93.75 [G Loss: 3.757654]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 948 [D Loss: 0.174480, acc] 94.53 [G Loss: 3.352749]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 949 [D Loss: 0.177328, acc] 92.97 [G Loss: 3.607974]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 950 [D Loss: 0.315475, acc] 85.16 [G Loss: 3.896992]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 951 [D Loss: 0.255864, acc] 87.50 [G Loss: 3.541186]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 952 [D Loss: 0.328219, acc] 84.38 [G Loss: 3.580325]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 953 [D Loss: 0.144741, acc] 96.09 [G Loss: 4.254101]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 954 [D Loss: 0.242064, acc] 89.84 [G Loss: 4.001666]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 955 [D Loss: 0.277484, acc] 86.72 [G Loss: 3.735785]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 956 [D Loss: 0.268314, acc] 87.50 [G Loss: 3.760334]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 957 [D Loss: 0.142301, acc] 95.31 [G Loss: 4.725237]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 958 [D Loss: 0.223325, acc] 92.97 [G Loss: 4.345958]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 959 [D Loss: 0.275751, acc] 85.16 [G Loss: 3.506306]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 960 [D Loss: 0.173772, acc] 91.41 [G Loss: 3.904371]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 961 [D Loss: 0.203611, acc] 93.75 [G Loss: 4.477282]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 962 [D Loss: 0.218090, acc] 89.84 [G Loss: 4.393368]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 963 [D Loss: 0.274088, acc] 87.50 [G Loss: 3.717555]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 964 [D Loss: 0.216178, acc] 91.41 [G Loss: 4.162349]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 965 [D Loss: 0.198507, acc] 90.62 [G Loss: 4.478599]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 966 [D Loss: 0.264531, acc] 88.28 [G Loss: 4.202844]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 967 [D Loss: 0.208351, acc] 91.41 [G Loss: 3.742198]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 968 [D Loss: 0.139914, acc] 96.09 [G Loss: 4.311637]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 969 [D Loss: 0.241603, acc] 90.62 [G Loss: 3.641282]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 970 [D Loss: 0.349280, acc] 81.25 [G Loss: 3.679622]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 971 [D Loss: 0.278658, acc] 85.94 [G Loss: 4.221210]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 972 [D Loss: 0.203215, acc] 92.19 [G Loss: 4.856573]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 973 [D Loss: 0.238065, acc] 91.41 [G Loss: 4.116595]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 974 [D Loss: 0.289579, acc] 89.84 [G Loss: 3.308105]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 975 [D Loss: 0.268291, acc] 88.28 [G Loss: 4.188956]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 976 [D Loss: 0.190660, acc] 91.41 [G Loss: 4.569350]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 977 [D Loss: 0.215373, acc] 91.41 [G Loss: 4.032782]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 978 [D Loss: 0.336735, acc] 83.59 [G Loss: 3.434722]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 979 [D Loss: 0.176759, acc] 92.97 [G Loss: 3.188548]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 980 [D Loss: 0.209754, acc] 91.41 [G Loss: 3.971131]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 981 [D Loss: 0.188603, acc] 95.31 [G Loss: 4.274668]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 982 [D Loss: 0.335636, acc] 87.50 [G Loss: 3.362172]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 983 [D Loss: 0.213335, acc] 90.62 [G Loss: 3.267535]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 984 [D Loss: 0.198214, acc] 93.75 [G Loss: 3.416847]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 985 [D Loss: 0.223360, acc] 91.41 [G Loss: 3.521351]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 986 [D Loss: 0.229938, acc] 91.41 [G Loss: 3.739136]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 987 [D Loss: 0.142169, acc] 95.31 [G Loss: 3.680436]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 988 [D Loss: 0.315498, acc] 87.50 [G Loss: 3.222987]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 989 [D Loss: 0.178180, acc] 92.97 [G Loss: 3.484777]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 990 [D Loss: 0.282384, acc] 87.50 [G Loss: 3.908333]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 991 [D Loss: 0.273088, acc] 87.50 [G Loss: 4.265188]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 992 [D Loss: 0.188620, acc] 93.75 [G Loss: 4.240088]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 993 [D Loss: 0.298746, acc] 87.50 [G Loss: 3.435870]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 994 [D Loss: 0.224541, acc] 92.19 [G Loss: 3.048951]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 995 [D Loss: 0.214353, acc] 89.06 [G Loss: 3.247860]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 996 [D Loss: 0.207155, acc] 91.41 [G Loss: 3.917757]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 997 [D Loss: 0.203610, acc] 90.62 [G Loss: 4.537004]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 998 [D Loss: 0.270055, acc] 88.28 [G Loss: 3.875411]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 999 [D Loss: 0.291198, acc] 89.06 [G Loss: 3.842108]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1000 [D Loss: 0.274440, acc] 85.94 [G Loss: 3.846047]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "0.00000006\n",
            "saved\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1001 [D Loss: 0.209625, acc] 92.19 [G Loss: 4.078135]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1002 [D Loss: 0.149254, acc] 94.53 [G Loss: 4.190796]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1003 [D Loss: 0.198505, acc] 92.19 [G Loss: 3.806502]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1004 [D Loss: 0.277284, acc] 89.06 [G Loss: 3.512825]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1005 [D Loss: 0.157302, acc] 94.53 [G Loss: 4.226715]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1006 [D Loss: 0.173173, acc] 96.09 [G Loss: 4.369557]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1007 [D Loss: 0.199912, acc] 90.62 [G Loss: 4.094882]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1008 [D Loss: 0.166178, acc] 94.53 [G Loss: 4.134102]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1009 [D Loss: 0.262520, acc] 89.84 [G Loss: 3.645849]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1010 [D Loss: 0.227817, acc] 89.84 [G Loss: 3.692056]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1011 [D Loss: 0.227410, acc] 90.62 [G Loss: 4.411593]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1012 [D Loss: 0.196352, acc] 93.75 [G Loss: 3.898502]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1013 [D Loss: 0.164591, acc] 92.97 [G Loss: 3.344779]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 1014 [D Loss: 0.248645, acc] 89.06 [G Loss: 3.732303]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1015 [D Loss: 0.202689, acc] 90.62 [G Loss: 3.722460]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1016 [D Loss: 0.245093, acc] 91.41 [G Loss: 3.635382]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1017 [D Loss: 0.234232, acc] 89.84 [G Loss: 3.702049]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1018 [D Loss: 0.310710, acc] 89.06 [G Loss: 3.745173]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1019 [D Loss: 0.266481, acc] 88.28 [G Loss: 3.372828]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1020 [D Loss: 0.237063, acc] 92.19 [G Loss: 3.227683]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1021 [D Loss: 0.209406, acc] 92.97 [G Loss: 4.056871]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1022 [D Loss: 0.241554, acc] 91.41 [G Loss: 3.343699]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1023 [D Loss: 0.191917, acc] 91.41 [G Loss: 3.632970]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1024 [D Loss: 0.198876, acc] 93.75 [G Loss: 3.746040]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1025 [D Loss: 0.253287, acc] 87.50 [G Loss: 3.226063]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1026 [D Loss: 0.298395, acc] 84.38 [G Loss: 2.919279]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1027 [D Loss: 0.298128, acc] 85.94 [G Loss: 3.734234]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1028 [D Loss: 0.233712, acc] 90.62 [G Loss: 4.376449]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1029 [D Loss: 0.268138, acc] 84.38 [G Loss: 2.835177]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1030 [D Loss: 0.210948, acc] 92.97 [G Loss: 3.086296]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1031 [D Loss: 0.253177, acc] 90.62 [G Loss: 3.927825]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1032 [D Loss: 0.232147, acc] 92.97 [G Loss: 3.586672]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1033 [D Loss: 0.326264, acc] 85.16 [G Loss: 3.477757]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1034 [D Loss: 0.164717, acc] 92.97 [G Loss: 3.402667]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1035 [D Loss: 0.329658, acc] 89.06 [G Loss: 3.415698]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1036 [D Loss: 0.248250, acc] 89.06 [G Loss: 3.829547]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1037 [D Loss: 0.353025, acc] 86.72 [G Loss: 3.647577]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1038 [D Loss: 0.255246, acc] 90.62 [G Loss: 3.626888]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1039 [D Loss: 0.276372, acc] 88.28 [G Loss: 3.134531]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1040 [D Loss: 0.355122, acc] 82.81 [G Loss: 3.046932]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1041 [D Loss: 0.263155, acc] 88.28 [G Loss: 3.924576]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1042 [D Loss: 0.344926, acc] 85.16 [G Loss: 3.586329]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 1043 [D Loss: 0.412658, acc] 82.81 [G Loss: 2.718705]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1044 [D Loss: 0.396168, acc] 78.91 [G Loss: 2.958529]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1045 [D Loss: 0.288846, acc] 88.28 [G Loss: 3.635993]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1046 [D Loss: 0.478036, acc] 81.25 [G Loss: 3.251286]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1047 [D Loss: 0.345233, acc] 85.94 [G Loss: 2.604564]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1048 [D Loss: 0.251543, acc] 90.62 [G Loss: 3.333828]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1049 [D Loss: 0.353070, acc] 83.59 [G Loss: 3.324375]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1050 [D Loss: 0.335002, acc] 84.38 [G Loss: 3.276801]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1051 [D Loss: 0.157509, acc] 95.31 [G Loss: 3.596997]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1052 [D Loss: 0.325747, acc] 87.50 [G Loss: 2.832228]\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "************ 1053 [D Loss: 0.363449, acc] 80.47 [G Loss: 3.008672]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1054 [D Loss: 0.205070, acc] 92.97 [G Loss: 3.956937]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1055 [D Loss: 0.274755, acc] 89.06 [G Loss: 3.647892]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1056 [D Loss: 0.377361, acc] 85.16 [G Loss: 2.750674]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1057 [D Loss: 0.550671, acc] 74.22 [G Loss: 2.726986]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1058 [D Loss: 0.233686, acc] 89.84 [G Loss: 3.915068]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1059 [D Loss: 0.363931, acc] 84.38 [G Loss: 3.790373]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1060 [D Loss: 0.292927, acc] 89.06 [G Loss: 3.078315]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 1061 [D Loss: 0.384467, acc] 80.47 [G Loss: 3.285963]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1062 [D Loss: 0.241271, acc] 88.28 [G Loss: 4.493331]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 1063 [D Loss: 0.220469, acc] 94.53 [G Loss: 4.157035]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1064 [D Loss: 0.420879, acc] 80.47 [G Loss: 2.674913]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1065 [D Loss: 0.389318, acc] 84.38 [G Loss: 2.481685]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1066 [D Loss: 0.188929, acc] 92.97 [G Loss: 4.098142]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1067 [D Loss: 0.354830, acc] 84.38 [G Loss: 4.239760]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1068 [D Loss: 0.227090, acc] 93.75 [G Loss: 3.083492]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1069 [D Loss: 0.352931, acc] 83.59 [G Loss: 3.119165]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1070 [D Loss: 0.286200, acc] 85.94 [G Loss: 3.773464]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1071 [D Loss: 0.224243, acc] 89.84 [G Loss: 4.227677]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1072 [D Loss: 0.342690, acc] 86.72 [G Loss: 3.293993]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1073 [D Loss: 0.306726, acc] 83.59 [G Loss: 2.662821]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1074 [D Loss: 0.278911, acc] 88.28 [G Loss: 3.457402]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1075 [D Loss: 0.241293, acc] 89.06 [G Loss: 3.921011]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1076 [D Loss: 0.325971, acc] 85.94 [G Loss: 3.469090]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1077 [D Loss: 0.404499, acc] 79.69 [G Loss: 3.174844]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1078 [D Loss: 0.272187, acc] 86.72 [G Loss: 3.894180]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1079 [D Loss: 0.312370, acc] 88.28 [G Loss: 4.112122]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1080 [D Loss: 0.427688, acc] 79.69 [G Loss: 3.004584]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1081 [D Loss: 0.265239, acc] 86.72 [G Loss: 3.017015]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1082 [D Loss: 0.261004, acc] 92.19 [G Loss: 4.069120]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1083 [D Loss: 0.303080, acc] 87.50 [G Loss: 3.655694]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1084 [D Loss: 0.231199, acc] 90.62 [G Loss: 2.625370]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1085 [D Loss: 0.399332, acc] 83.59 [G Loss: 3.376152]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1086 [D Loss: 0.192357, acc] 95.31 [G Loss: 4.320520]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1087 [D Loss: 0.379165, acc] 81.25 [G Loss: 2.959173]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1088 [D Loss: 0.389713, acc] 81.25 [G Loss: 2.204087]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1089 [D Loss: 0.363071, acc] 79.69 [G Loss: 3.612410]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1090 [D Loss: 0.182336, acc] 95.31 [G Loss: 4.570474]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1091 [D Loss: 0.511171, acc] 77.34 [G Loss: 2.620963]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1092 [D Loss: 0.428332, acc] 82.81 [G Loss: 1.803324]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1093 [D Loss: 0.309758, acc] 87.50 [G Loss: 3.150582]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1094 [D Loss: 0.219335, acc] 89.84 [G Loss: 4.643201]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1095 [D Loss: 0.249545, acc] 90.62 [G Loss: 4.156495]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1096 [D Loss: 0.373080, acc] 81.25 [G Loss: 2.305984]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1097 [D Loss: 0.431766, acc] 78.91 [G Loss: 2.537764]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1098 [D Loss: 0.160918, acc] 96.09 [G Loss: 3.897302]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1099 [D Loss: 0.222484, acc] 91.41 [G Loss: 4.141273]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1100 [D Loss: 0.381202, acc] 84.38 [G Loss: 3.327740]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1101 [D Loss: 0.277897, acc] 88.28 [G Loss: 3.204441]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1102 [D Loss: 0.192895, acc] 92.97 [G Loss: 3.457174]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1103 [D Loss: 0.284730, acc] 89.06 [G Loss: 3.575170]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 1104 [D Loss: 0.227359, acc] 90.62 [G Loss: 3.548730]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1105 [D Loss: 0.233002, acc] 90.62 [G Loss: 3.404211]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1106 [D Loss: 0.315318, acc] 87.50 [G Loss: 3.286549]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1107 [D Loss: 0.380560, acc] 81.25 [G Loss: 2.721701]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1108 [D Loss: 0.355570, acc] 81.25 [G Loss: 3.786277]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1109 [D Loss: 0.256381, acc] 88.28 [G Loss: 3.466585]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1110 [D Loss: 0.444645, acc] 76.56 [G Loss: 2.525133]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 1111 [D Loss: 0.302292, acc] 85.16 [G Loss: 2.786282]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1112 [D Loss: 0.335098, acc] 83.59 [G Loss: 3.392646]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1113 [D Loss: 0.396200, acc] 83.59 [G Loss: 3.459546]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1114 [D Loss: 0.242705, acc] 90.62 [G Loss: 2.891594]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1115 [D Loss: 0.288921, acc] 87.50 [G Loss: 2.924653]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1116 [D Loss: 0.413844, acc] 80.47 [G Loss: 2.664830]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1117 [D Loss: 0.347837, acc] 83.59 [G Loss: 3.351355]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1118 [D Loss: 0.252272, acc] 92.97 [G Loss: 3.555576]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1119 [D Loss: 0.421061, acc] 82.81 [G Loss: 2.880019]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1120 [D Loss: 0.341555, acc] 86.72 [G Loss: 2.498205]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1121 [D Loss: 0.426113, acc] 78.91 [G Loss: 3.313106]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1122 [D Loss: 0.276520, acc] 89.84 [G Loss: 4.059011]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1123 [D Loss: 0.525318, acc] 79.69 [G Loss: 2.857215]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1124 [D Loss: 0.430021, acc] 78.12 [G Loss: 2.213743]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1125 [D Loss: 0.384610, acc] 81.25 [G Loss: 2.936708]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1126 [D Loss: 0.285002, acc] 89.84 [G Loss: 3.818720]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1127 [D Loss: 0.381033, acc] 89.06 [G Loss: 4.106197]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1128 [D Loss: 0.407376, acc] 83.59 [G Loss: 2.594259]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1129 [D Loss: 0.350020, acc] 85.16 [G Loss: 2.523543]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1130 [D Loss: 0.264007, acc] 88.28 [G Loss: 3.685745]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1131 [D Loss: 0.274329, acc] 88.28 [G Loss: 4.184453]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1132 [D Loss: 0.246498, acc] 92.19 [G Loss: 3.428396]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1133 [D Loss: 0.312882, acc] 88.28 [G Loss: 2.556670]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1134 [D Loss: 0.329325, acc] 83.59 [G Loss: 3.157225]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1135 [D Loss: 0.193475, acc] 93.75 [G Loss: 3.568051]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1136 [D Loss: 0.306565, acc] 89.84 [G Loss: 3.502980]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1137 [D Loss: 0.317719, acc] 85.94 [G Loss: 2.936476]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1138 [D Loss: 0.291932, acc] 89.84 [G Loss: 2.001247]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1139 [D Loss: 0.316833, acc] 82.81 [G Loss: 2.931190]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1140 [D Loss: 0.235470, acc] 92.19 [G Loss: 3.328996]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1141 [D Loss: 0.465367, acc] 81.25 [G Loss: 2.354099]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1142 [D Loss: 0.347029, acc] 85.94 [G Loss: 2.158166]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1143 [D Loss: 0.401528, acc] 83.59 [G Loss: 2.628938]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1144 [D Loss: 0.370614, acc] 82.81 [G Loss: 3.330466]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1145 [D Loss: 0.267491, acc] 90.62 [G Loss: 3.406648]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1146 [D Loss: 0.325631, acc] 86.72 [G Loss: 2.614069]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1147 [D Loss: 0.472352, acc] 78.12 [G Loss: 2.550010]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1148 [D Loss: 0.307131, acc] 86.72 [G Loss: 3.229666]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1149 [D Loss: 0.303629, acc] 84.38 [G Loss: 3.504102]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1150 [D Loss: 0.361468, acc] 84.38 [G Loss: 2.875961]\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "************ 1151 [D Loss: 0.436629, acc] 80.47 [G Loss: 2.217235]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1152 [D Loss: 0.318046, acc] 86.72 [G Loss: 2.766987]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1153 [D Loss: 0.250914, acc] 85.94 [G Loss: 3.677590]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1154 [D Loss: 0.287435, acc] 85.94 [G Loss: 3.823978]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 1155 [D Loss: 0.433365, acc] 80.47 [G Loss: 2.823488]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1156 [D Loss: 0.398262, acc] 82.81 [G Loss: 2.745031]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1157 [D Loss: 0.394753, acc] 84.38 [G Loss: 3.072449]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1158 [D Loss: 0.364226, acc] 81.25 [G Loss: 3.126052]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1159 [D Loss: 0.318091, acc] 86.72 [G Loss: 2.850121]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1160 [D Loss: 0.347716, acc] 83.59 [G Loss: 2.507678]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 1161 [D Loss: 0.272009, acc] 85.94 [G Loss: 2.953923]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1162 [D Loss: 0.395885, acc] 80.47 [G Loss: 2.630112]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1163 [D Loss: 0.520238, acc] 74.22 [G Loss: 2.644633]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1164 [D Loss: 0.402009, acc] 78.91 [G Loss: 2.495658]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1165 [D Loss: 0.347713, acc] 87.50 [G Loss: 3.012008]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1166 [D Loss: 0.528552, acc] 71.88 [G Loss: 3.084212]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1167 [D Loss: 0.447699, acc] 77.34 [G Loss: 3.180001]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1168 [D Loss: 0.565686, acc] 76.56 [G Loss: 2.250652]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1169 [D Loss: 0.291547, acc] 90.62 [G Loss: 2.443067]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1170 [D Loss: 0.412142, acc] 78.12 [G Loss: 2.827925]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1171 [D Loss: 0.382349, acc] 82.81 [G Loss: 2.689584]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1172 [D Loss: 0.410899, acc] 79.69 [G Loss: 2.433013]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1173 [D Loss: 0.416561, acc] 82.03 [G Loss: 2.313139]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1174 [D Loss: 0.513086, acc] 78.91 [G Loss: 2.597329]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1175 [D Loss: 0.326272, acc] 85.94 [G Loss: 3.513841]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1176 [D Loss: 0.449295, acc] 82.81 [G Loss: 3.304533]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1177 [D Loss: 0.502938, acc] 76.56 [G Loss: 1.902692]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1178 [D Loss: 0.413385, acc] 83.59 [G Loss: 2.295060]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1179 [D Loss: 0.288960, acc] 88.28 [G Loss: 3.357213]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1180 [D Loss: 0.394533, acc] 82.81 [G Loss: 3.840895]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1181 [D Loss: 0.374970, acc] 84.38 [G Loss: 2.870877]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1182 [D Loss: 0.445288, acc] 77.34 [G Loss: 2.519658]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1183 [D Loss: 0.417478, acc] 78.91 [G Loss: 2.779316]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1184 [D Loss: 0.282345, acc] 90.62 [G Loss: 3.435262]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 1185 [D Loss: 0.381400, acc] 85.16 [G Loss: 3.365795]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1186 [D Loss: 0.396618, acc] 82.03 [G Loss: 3.088151]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1187 [D Loss: 0.387567, acc] 84.38 [G Loss: 2.467891]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1188 [D Loss: 0.406647, acc] 79.69 [G Loss: 2.553501]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1189 [D Loss: 0.340171, acc] 85.16 [G Loss: 3.136202]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1190 [D Loss: 0.336217, acc] 89.06 [G Loss: 3.528971]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1191 [D Loss: 0.342011, acc] 85.94 [G Loss: 2.846952]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1192 [D Loss: 0.403849, acc] 85.16 [G Loss: 2.283417]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1193 [D Loss: 0.365056, acc] 82.81 [G Loss: 2.485256]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 1194 [D Loss: 0.417761, acc] 82.03 [G Loss: 2.263798]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1195 [D Loss: 0.448403, acc] 78.12 [G Loss: 2.471490]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1196 [D Loss: 0.363133, acc] 82.81 [G Loss: 3.046655]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1197 [D Loss: 0.452392, acc] 77.34 [G Loss: 3.367241]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1198 [D Loss: 0.506653, acc] 75.00 [G Loss: 2.504612]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1199 [D Loss: 0.430984, acc] 79.69 [G Loss: 1.925429]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1200 [D Loss: 0.347343, acc] 85.94 [G Loss: 2.688621]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "0.00000007\n",
            "saved\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1201 [D Loss: 0.286850, acc] 85.16 [G Loss: 3.121762]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1202 [D Loss: 0.470242, acc] 75.00 [G Loss: 2.855292]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1203 [D Loss: 0.374311, acc] 85.94 [G Loss: 2.400940]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 1204 [D Loss: 0.388668, acc] 80.47 [G Loss: 2.543095]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1205 [D Loss: 0.464731, acc] 78.12 [G Loss: 2.470211]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1206 [D Loss: 0.293387, acc] 92.19 [G Loss: 3.102802]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1207 [D Loss: 0.476972, acc] 78.12 [G Loss: 2.995544]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1208 [D Loss: 0.438893, acc] 75.78 [G Loss: 2.249962]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1209 [D Loss: 0.573710, acc] 78.12 [G Loss: 2.252272]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1210 [D Loss: 0.359202, acc] 81.25 [G Loss: 3.152246]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1211 [D Loss: 0.353146, acc] 84.38 [G Loss: 2.995745]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1212 [D Loss: 0.547055, acc] 77.34 [G Loss: 2.622847]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1213 [D Loss: 0.480978, acc] 78.12 [G Loss: 2.612757]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1214 [D Loss: 0.436736, acc] 83.59 [G Loss: 2.716569]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1215 [D Loss: 0.360855, acc] 86.72 [G Loss: 2.745324]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1216 [D Loss: 0.529130, acc] 71.88 [G Loss: 2.809809]\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "************ 1217 [D Loss: 0.372010, acc] 84.38 [G Loss: 3.129347]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1218 [D Loss: 0.333995, acc] 86.72 [G Loss: 2.660252]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1219 [D Loss: 0.434549, acc] 77.34 [G Loss: 2.960320]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1220 [D Loss: 0.415930, acc] 79.69 [G Loss: 3.271304]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1221 [D Loss: 0.578987, acc] 73.44 [G Loss: 2.441739]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 1222 [D Loss: 0.435826, acc] 78.91 [G Loss: 2.536308]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1223 [D Loss: 0.445295, acc] 82.03 [G Loss: 2.968048]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1224 [D Loss: 0.447547, acc] 84.38 [G Loss: 2.651107]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1225 [D Loss: 0.398438, acc] 80.47 [G Loss: 2.424213]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1226 [D Loss: 0.391129, acc] 80.47 [G Loss: 2.256399]\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "************ 1227 [D Loss: 0.445928, acc] 78.91 [G Loss: 2.460907]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1228 [D Loss: 0.329737, acc] 85.94 [G Loss: 2.661256]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1229 [D Loss: 0.355848, acc] 86.72 [G Loss: 2.959352]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1230 [D Loss: 0.664169, acc] 71.88 [G Loss: 2.562227]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1231 [D Loss: 0.415790, acc] 81.25 [G Loss: 2.375746]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 1232 [D Loss: 0.413175, acc] 83.59 [G Loss: 2.693798]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1233 [D Loss: 0.344501, acc] 82.81 [G Loss: 2.778727]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1234 [D Loss: 0.567090, acc] 80.47 [G Loss: 2.524521]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1235 [D Loss: 0.407789, acc] 82.03 [G Loss: 2.419421]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1236 [D Loss: 0.412729, acc] 79.69 [G Loss: 2.539903]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1237 [D Loss: 0.527906, acc] 76.56 [G Loss: 2.559476]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1238 [D Loss: 0.446998, acc] 79.69 [G Loss: 2.312526]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1239 [D Loss: 0.466217, acc] 80.47 [G Loss: 2.289032]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1240 [D Loss: 0.483152, acc] 75.00 [G Loss: 2.331187]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1241 [D Loss: 0.275991, acc] 86.72 [G Loss: 2.904720]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1242 [D Loss: 0.516232, acc] 75.78 [G Loss: 2.443768]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1243 [D Loss: 0.296587, acc] 85.94 [G Loss: 2.424836]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1244 [D Loss: 0.435263, acc] 80.47 [G Loss: 2.351662]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1245 [D Loss: 0.440361, acc] 78.12 [G Loss: 2.381630]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1246 [D Loss: 0.429244, acc] 78.91 [G Loss: 2.717895]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1247 [D Loss: 0.348849, acc] 86.72 [G Loss: 2.741531]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1248 [D Loss: 0.324298, acc] 89.06 [G Loss: 2.415683]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1249 [D Loss: 0.418034, acc] 80.47 [G Loss: 2.421521]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1250 [D Loss: 0.442012, acc] 79.69 [G Loss: 2.878482]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1251 [D Loss: 0.382194, acc] 84.38 [G Loss: 2.631912]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1252 [D Loss: 0.287804, acc] 87.50 [G Loss: 2.789390]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1253 [D Loss: 0.508247, acc] 75.00 [G Loss: 2.250944]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1254 [D Loss: 0.344098, acc] 87.50 [G Loss: 2.455727]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1255 [D Loss: 0.260917, acc] 89.06 [G Loss: 2.882174]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1256 [D Loss: 0.293177, acc] 86.72 [G Loss: 3.078006]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1257 [D Loss: 0.472801, acc] 78.12 [G Loss: 2.141714]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1258 [D Loss: 0.393888, acc] 80.47 [G Loss: 2.270069]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1259 [D Loss: 0.317648, acc] 85.16 [G Loss: 2.317436]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1260 [D Loss: 0.260164, acc] 93.75 [G Loss: 2.263949]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1261 [D Loss: 0.325928, acc] 85.16 [G Loss: 2.636145]\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "************ 1262 [D Loss: 0.380993, acc] 84.38 [G Loss: 2.562721]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1263 [D Loss: 0.348754, acc] 86.72 [G Loss: 2.214304]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1264 [D Loss: 0.297789, acc] 86.72 [G Loss: 2.688355]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1265 [D Loss: 0.418083, acc] 78.91 [G Loss: 2.659805]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1266 [D Loss: 0.282801, acc] 89.06 [G Loss: 2.686044]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1267 [D Loss: 0.462316, acc] 80.47 [G Loss: 2.490540]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1268 [D Loss: 0.360500, acc] 84.38 [G Loss: 2.947346]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1269 [D Loss: 0.331017, acc] 84.38 [G Loss: 2.930029]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1270 [D Loss: 0.390564, acc] 82.03 [G Loss: 3.030536]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1271 [D Loss: 0.487594, acc] 75.78 [G Loss: 2.707222]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1272 [D Loss: 0.296684, acc] 89.06 [G Loss: 2.466845]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1273 [D Loss: 0.536905, acc] 71.09 [G Loss: 2.254733]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1274 [D Loss: 0.336324, acc] 83.59 [G Loss: 2.860579]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1275 [D Loss: 0.427369, acc] 75.78 [G Loss: 2.838919]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1276 [D Loss: 0.477117, acc] 77.34 [G Loss: 2.408922]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1277 [D Loss: 0.511563, acc] 78.91 [G Loss: 1.959302]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1278 [D Loss: 0.406868, acc] 85.94 [G Loss: 2.356618]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1279 [D Loss: 0.484357, acc] 78.91 [G Loss: 2.807359]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1280 [D Loss: 0.510614, acc] 76.56 [G Loss: 2.187584]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1281 [D Loss: 0.548848, acc] 74.22 [G Loss: 2.514526]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1282 [D Loss: 0.395952, acc] 80.47 [G Loss: 2.709268]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1283 [D Loss: 0.578199, acc] 68.75 [G Loss: 2.307850]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1284 [D Loss: 0.501633, acc] 73.44 [G Loss: 2.569901]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1285 [D Loss: 0.480328, acc] 75.00 [G Loss: 2.665323]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1286 [D Loss: 0.365080, acc] 85.16 [G Loss: 2.549907]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1287 [D Loss: 0.467641, acc] 79.69 [G Loss: 1.893087]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1288 [D Loss: 0.592661, acc] 73.44 [G Loss: 1.758358]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1289 [D Loss: 0.425775, acc] 77.34 [G Loss: 2.547648]\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "************ 1290 [D Loss: 0.420042, acc] 79.69 [G Loss: 3.120516]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1291 [D Loss: 0.508796, acc] 71.88 [G Loss: 2.321449]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1292 [D Loss: 0.516735, acc] 74.22 [G Loss: 1.866719]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1293 [D Loss: 0.627838, acc] 66.41 [G Loss: 2.055617]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1294 [D Loss: 0.449269, acc] 75.00 [G Loss: 2.553047]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1295 [D Loss: 0.382069, acc] 82.03 [G Loss: 3.095247]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1296 [D Loss: 0.435301, acc] 78.91 [G Loss: 3.032604]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1297 [D Loss: 0.518751, acc] 76.56 [G Loss: 2.330380]\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "************ 1298 [D Loss: 0.465846, acc] 75.78 [G Loss: 2.588154]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1299 [D Loss: 0.335052, acc] 84.38 [G Loss: 3.241261]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1300 [D Loss: 0.419209, acc] 78.12 [G Loss: 3.156132]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1301 [D Loss: 0.450637, acc] 79.69 [G Loss: 2.551015]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1302 [D Loss: 0.457873, acc] 77.34 [G Loss: 2.243454]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1303 [D Loss: 0.412231, acc] 82.81 [G Loss: 2.399281]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1304 [D Loss: 0.377624, acc] 85.16 [G Loss: 3.020231]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1305 [D Loss: 0.370373, acc] 80.47 [G Loss: 2.644997]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1306 [D Loss: 0.351022, acc] 82.81 [G Loss: 2.718081]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1307 [D Loss: 0.409458, acc] 82.03 [G Loss: 2.836073]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1308 [D Loss: 0.386056, acc] 79.69 [G Loss: 2.455749]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1309 [D Loss: 0.383085, acc] 85.16 [G Loss: 2.105519]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1310 [D Loss: 0.448896, acc] 78.12 [G Loss: 2.311520]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1311 [D Loss: 0.494910, acc] 76.56 [G Loss: 2.738251]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1312 [D Loss: 0.409156, acc] 81.25 [G Loss: 2.946511]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1313 [D Loss: 0.405671, acc] 81.25 [G Loss: 2.100807]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1314 [D Loss: 0.440847, acc] 77.34 [G Loss: 2.057840]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1315 [D Loss: 0.352555, acc] 85.94 [G Loss: 2.317348]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1316 [D Loss: 0.395936, acc] 83.59 [G Loss: 2.416728]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1317 [D Loss: 0.439371, acc] 80.47 [G Loss: 2.295184]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1318 [D Loss: 0.452814, acc] 77.34 [G Loss: 1.865568]\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "************ 1319 [D Loss: 0.471505, acc] 76.56 [G Loss: 2.234941]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1320 [D Loss: 0.470057, acc] 80.47 [G Loss: 2.576551]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1321 [D Loss: 0.487963, acc] 77.34 [G Loss: 2.528811]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1322 [D Loss: 0.644953, acc] 64.84 [G Loss: 1.916402]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1323 [D Loss: 0.464337, acc] 77.34 [G Loss: 2.085531]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1324 [D Loss: 0.385515, acc] 85.16 [G Loss: 2.448635]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1325 [D Loss: 0.341390, acc] 85.94 [G Loss: 3.004103]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1326 [D Loss: 0.654447, acc] 75.78 [G Loss: 1.944340]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1327 [D Loss: 0.584137, acc] 66.41 [G Loss: 1.571167]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1328 [D Loss: 0.439807, acc] 79.69 [G Loss: 2.358763]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1329 [D Loss: 0.381711, acc] 85.94 [G Loss: 2.958291]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1330 [D Loss: 0.323452, acc] 89.06 [G Loss: 3.083716]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1331 [D Loss: 0.452798, acc] 81.25 [G Loss: 2.401476]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1332 [D Loss: 0.443707, acc] 75.00 [G Loss: 2.062556]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1333 [D Loss: 0.407034, acc] 78.12 [G Loss: 2.234024]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1334 [D Loss: 0.244493, acc] 91.41 [G Loss: 3.324267]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1335 [D Loss: 0.374616, acc] 85.94 [G Loss: 2.970207]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1336 [D Loss: 0.381886, acc] 82.03 [G Loss: 2.581791]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 1337 [D Loss: 0.409000, acc] 82.03 [G Loss: 1.890313]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1338 [D Loss: 0.370582, acc] 86.72 [G Loss: 2.477948]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1339 [D Loss: 0.333922, acc] 88.28 [G Loss: 2.856269]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 1340 [D Loss: 0.397649, acc] 81.25 [G Loss: 2.710100]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1341 [D Loss: 0.532347, acc] 67.97 [G Loss: 2.510134]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 1342 [D Loss: 0.498372, acc] 72.66 [G Loss: 2.406401]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1343 [D Loss: 0.425333, acc] 78.91 [G Loss: 2.867160]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1344 [D Loss: 0.451531, acc] 82.03 [G Loss: 2.599025]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1345 [D Loss: 0.425611, acc] 79.69 [G Loss: 2.033785]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 1346 [D Loss: 0.378657, acc] 79.69 [G Loss: 2.131171]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1347 [D Loss: 0.390891, acc] 83.59 [G Loss: 2.310609]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1348 [D Loss: 0.395332, acc] 85.16 [G Loss: 2.399900]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1349 [D Loss: 0.408384, acc] 81.25 [G Loss: 2.545972]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1350 [D Loss: 0.511195, acc] 73.44 [G Loss: 2.300774]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1351 [D Loss: 0.458253, acc] 76.56 [G Loss: 2.368464]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1352 [D Loss: 0.481392, acc] 76.56 [G Loss: 2.466492]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1353 [D Loss: 0.676248, acc] 67.97 [G Loss: 2.132679]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1354 [D Loss: 0.515857, acc] 75.00 [G Loss: 2.201960]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1355 [D Loss: 0.409545, acc] 81.25 [G Loss: 2.103005]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1356 [D Loss: 0.442072, acc] 78.91 [G Loss: 2.043016]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1357 [D Loss: 0.492124, acc] 74.22 [G Loss: 2.076999]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1358 [D Loss: 0.498394, acc] 78.12 [G Loss: 2.097297]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1359 [D Loss: 0.457127, acc] 78.91 [G Loss: 2.439539]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1360 [D Loss: 0.376981, acc] 82.81 [G Loss: 2.716763]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1361 [D Loss: 0.372230, acc] 85.16 [G Loss: 2.164498]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1362 [D Loss: 0.394824, acc] 85.16 [G Loss: 1.987534]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1363 [D Loss: 0.537524, acc] 73.44 [G Loss: 2.102398]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1364 [D Loss: 0.308117, acc] 89.84 [G Loss: 2.604853]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1365 [D Loss: 0.604182, acc] 71.09 [G Loss: 2.231626]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1366 [D Loss: 0.617852, acc] 70.31 [G Loss: 1.944659]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1367 [D Loss: 0.458058, acc] 80.47 [G Loss: 2.017668]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1368 [D Loss: 0.371476, acc] 83.59 [G Loss: 2.263190]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1369 [D Loss: 0.535742, acc] 73.44 [G Loss: 2.469296]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1370 [D Loss: 0.453576, acc] 78.91 [G Loss: 2.503987]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1371 [D Loss: 0.506772, acc] 79.69 [G Loss: 2.140641]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1372 [D Loss: 0.529661, acc] 76.56 [G Loss: 1.970677]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1373 [D Loss: 0.403969, acc] 79.69 [G Loss: 2.435208]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1374 [D Loss: 0.411634, acc] 81.25 [G Loss: 2.641160]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1375 [D Loss: 0.447161, acc] 78.91 [G Loss: 2.658582]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 1376 [D Loss: 0.404233, acc] 80.47 [G Loss: 2.330148]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1377 [D Loss: 0.409480, acc] 82.03 [G Loss: 2.229100]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1378 [D Loss: 0.347181, acc] 85.94 [G Loss: 2.034517]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "************ 1379 [D Loss: 0.368653, acc] 85.94 [G Loss: 2.912881]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1380 [D Loss: 0.324199, acc] 88.28 [G Loss: 2.573338]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1381 [D Loss: 0.554528, acc] 73.44 [G Loss: 2.204785]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1382 [D Loss: 0.425544, acc] 82.03 [G Loss: 2.049761]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1383 [D Loss: 0.448567, acc] 79.69 [G Loss: 2.547062]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1384 [D Loss: 0.416836, acc] 81.25 [G Loss: 2.334175]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1385 [D Loss: 0.496178, acc] 78.12 [G Loss: 2.158202]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1386 [D Loss: 0.358575, acc] 81.25 [G Loss: 2.272660]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1387 [D Loss: 0.395441, acc] 82.03 [G Loss: 2.493132]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1388 [D Loss: 0.544211, acc] 75.00 [G Loss: 2.281265]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1389 [D Loss: 0.437648, acc] 81.25 [G Loss: 2.717688]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1390 [D Loss: 0.407885, acc] 83.59 [G Loss: 2.933525]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1391 [D Loss: 0.544001, acc] 81.25 [G Loss: 2.479458]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1392 [D Loss: 0.448563, acc] 76.56 [G Loss: 1.818212]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1393 [D Loss: 0.535878, acc] 72.66 [G Loss: 1.613316]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1394 [D Loss: 0.477150, acc] 75.00 [G Loss: 2.155571]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1395 [D Loss: 0.420564, acc] 81.25 [G Loss: 2.658562]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1396 [D Loss: 0.431380, acc] 81.25 [G Loss: 2.758368]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1397 [D Loss: 0.458126, acc] 78.91 [G Loss: 2.062778]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 1398 [D Loss: 0.486574, acc] 77.34 [G Loss: 1.572517]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1399 [D Loss: 0.476373, acc] 80.47 [G Loss: 2.102465]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1400 [D Loss: 0.396376, acc] 80.47 [G Loss: 2.600421]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "0.00000008\n",
            "saved\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "************ 1401 [D Loss: 0.385328, acc] 83.59 [G Loss: 2.635400]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1402 [D Loss: 0.493674, acc] 82.03 [G Loss: 2.288099]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1403 [D Loss: 0.550338, acc] 73.44 [G Loss: 1.739227]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1404 [D Loss: 0.331846, acc] 87.50 [G Loss: 2.544848]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "************ 1405 [D Loss: 0.307030, acc] 87.50 [G Loss: 3.118144]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1406 [D Loss: 0.705585, acc] 68.75 [G Loss: 2.012002]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1407 [D Loss: 0.445204, acc] 78.91 [G Loss: 2.150018]\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "************ 1408 [D Loss: 0.461170, acc] 75.78 [G Loss: 2.460466]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1409 [D Loss: 0.281955, acc] 89.84 [G Loss: 2.861008]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1410 [D Loss: 0.468461, acc] 78.12 [G Loss: 2.166088]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "************ 1411 [D Loss: 0.454974, acc] 83.59 [G Loss: 2.038795]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1412 [D Loss: 0.450414, acc] 78.91 [G Loss: 2.286000]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1413 [D Loss: 0.345011, acc] 85.94 [G Loss: 2.131300]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1414 [D Loss: 0.371272, acc] 85.94 [G Loss: 2.281665]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1415 [D Loss: 0.476693, acc] 78.91 [G Loss: 2.509547]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1416 [D Loss: 0.461101, acc] 80.47 [G Loss: 2.369619]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "************ 1417 [D Loss: 0.437439, acc] 82.81 [G Loss: 1.702821]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1418 [D Loss: 0.419540, acc] 84.38 [G Loss: 2.249533]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1419 [D Loss: 0.458075, acc] 80.47 [G Loss: 2.177931]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1420 [D Loss: 0.451506, acc] 77.34 [G Loss: 2.171423]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1421 [D Loss: 0.427436, acc] 83.59 [G Loss: 1.980220]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1422 [D Loss: 0.523995, acc] 73.44 [G Loss: 2.024024]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1423 [D Loss: 0.460925, acc] 78.91 [G Loss: 2.272444]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1424 [D Loss: 0.467180, acc] 79.69 [G Loss: 2.205955]\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "************ 1425 [D Loss: 0.438524, acc] 81.25 [G Loss: 2.275642]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1426 [D Loss: 0.505868, acc] 76.56 [G Loss: 1.916029]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1427 [D Loss: 0.560442, acc] 72.66 [G Loss: 2.074252]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1428 [D Loss: 0.390802, acc] 83.59 [G Loss: 2.464262]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 1429 [D Loss: 0.438502, acc] 75.00 [G Loss: 2.424850]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1430 [D Loss: 0.514242, acc] 72.66 [G Loss: 1.819700]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1431 [D Loss: 0.489886, acc] 75.00 [G Loss: 1.749560]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1432 [D Loss: 0.467307, acc] 75.78 [G Loss: 1.859904]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1433 [D Loss: 0.345029, acc] 84.38 [G Loss: 2.183583]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1434 [D Loss: 0.565538, acc] 77.34 [G Loss: 2.229066]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1435 [D Loss: 0.537208, acc] 73.44 [G Loss: 2.127484]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1436 [D Loss: 0.566264, acc] 71.09 [G Loss: 1.979614]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1437 [D Loss: 0.511068, acc] 80.47 [G Loss: 2.002814]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1438 [D Loss: 0.451096, acc] 77.34 [G Loss: 2.012954]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "************ 1439 [D Loss: 0.597954, acc] 71.88 [G Loss: 2.125525]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "************ 1440 [D Loss: 0.367238, acc] 83.59 [G Loss: 2.716252]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1441 [D Loss: 0.532492, acc] 76.56 [G Loss: 2.469911]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1442 [D Loss: 0.450644, acc] 78.91 [G Loss: 2.470941]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "************ 1443 [D Loss: 0.406958, acc] 82.81 [G Loss: 2.290412]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1444 [D Loss: 0.417287, acc] 78.91 [G Loss: 2.320947]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1445 [D Loss: 0.338201, acc] 87.50 [G Loss: 1.955156]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1446 [D Loss: 0.479820, acc] 78.91 [G Loss: 1.819613]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "************ 1447 [D Loss: 0.404764, acc] 78.91 [G Loss: 2.134613]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1448 [D Loss: 0.526119, acc] 77.34 [G Loss: 2.347192]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "************ 1449 [D Loss: 0.369126, acc] 84.38 [G Loss: 2.466689]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "************ 1450 [D Loss: 0.369925, acc] 82.81 [G Loss: 2.230054]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "************ 1451 [D Loss: 0.467091, acc] 74.22 [G Loss: 2.035697]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-95aea51708f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-95aea51708f5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2315\u001b[0m                     )\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   2318\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1260\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2292\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   2293\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 2294\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2295\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5503\u001b[0m         use_legacy_function=use_legacy_function)\n\u001b[1;32m   5504\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5505\u001b[0;31m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0m\u001b[1;32m   5506\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5507\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   3458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3460\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3461\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MapDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m         \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-jSQoN1Azl"
      },
      "source": [
        "### **8) Making GIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPShgQpg1EMy"
      },
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('generated_images/*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh37uv1torG5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}